gam_mod <- fit(gam_spec,
Limit ~ s(Income) + Cards + Age + Education + Gender + Student + Married + Ethnicity + s(Balance),
data = Credit)
# Present the GAM model result
gam_mod %>% pluck('fit') %>% summary()
# Compare the result with the linear GAMs model
gam_mod_temp <- fit(gam_spec,
Limit ~ Income + Cards + Age + Education + Gender + Student + Married + Ethnicity + Balance,
data = Credit)
gam_mod_temp %>% pluck('fit') %>% summary()
# Visualize the evaluation plots for GAMs
par(mfrow = c(2, 3))
gam_mod %>% pluck('fit') %>% mgcv::gam.check()
gam_mod %>% pluck('fit') %>% plot()
# Visualize the residual plot for GAMs
gam_output <- Credit %>%
bind_cols(predict(gam_mod, new_data = Credit)) %>%
mutate(resid = Limit - .pred)
ggplot(gam_output, aes(x = Income, y = resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept = 0, color = "red") +
theme_classic()
ggplot(gam_output, aes(x = Education, y = resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept = 0, color = "red") +
theme_classic()
ggplot(gam_output, aes(x = Balance, y = resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept = 0, color = "red") +
theme_classic()
ggplot(gam_output, aes(x = .pred, y = resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept = 0, color = "red") +
theme_classic()
#  Calculate and collect CV metrics for GAMs
gam_spec2 <-
linear_reg() %>%
set_engine(engine = 'lm') %>%
set_mode('regression')
gam_rec <- rec_r %>%
step_naomit(all_numeric_predictors(), skip = FALSE) %>%
step_ns(Income, deg_free = 1.481) %>%
step_ns(Balance, deg_free = 8.426)
gam_wf <- workflow() %>%
add_model(gam_spec2) %>%
add_recipe(gam_rec)
gam_output <- fit_resamples(gam_wf,
resamples = Credit_cv5,
metrics = metric_set(mae, rmse, rsq))
gam_output %>% collect_metrics()
regreDF <- lm_output %>%
collect_metrics() %>%
mutate(OLS = mean,
OLS_std = std_err) %>%
select(.metric, OLS, OLS_std) %>%
left_join(lasso_output %>%
mutate(LASSO = mean,
LASSO_std = std_err) %>%
select(.metric, LASSO, LASSO_std),
by = ".metric") %>%
left_join(gam_output %>%
collect_metrics() %>%
mutate(GAMs = mean,
GAMs_std = std_err) %>%
select(.metric, GAMs, GAMs_std),
by = ".metric")
regreDF
gam_mod %>% pluck('fit') %>% summary()
Titanic <- read_csv("Data/train.csv")
# Data Cleaning
Titanic_clean <- Titanic %>%
select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
na.omit()
head(Titanic_clean)
# Create k5 CV for the data
titanic_c_cv5 <- vfold_cv(Titanic_clean, v = 5)
# Visualize the outcome variable
Titanic_clean %>%
ggplot(aes(x = as.factor(Survived))) +
geom_bar() +
xlab("Survived")
# Process the data and make the recipe for classification
titanic_c <- Titanic_clean %>%
mutate(Survived = as.factor(Survived))
rec_c <- recipe(Survived ~ ., data = titanic_c) %>%
step_nzv(all_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_corr(all_numeric_predictors())
# Helper method to calculate Sensitivity, Specificity, and Accuracy
helper_confusion <- function(x) {
return(list(
"Sensitivty" = x$table[1] / (x$table[1] + x$table[2]),
"Specificity" = x$table[4] / (x$table[3] + x$table[4]),
"Accuracy" = (x$table[1] + x$table[4]) / (x$table[1] + x$table[2] + x$table[3] + x$table[4])
))
}
# model spec for RF
rf_spec <- rand_forest() %>%
set_engine(engine = 'ranger') %>%
set_args(
mtry = NULL,
trees = 100,
min_n = 2,
probability = FALSE,
importance = 'impurity'
) %>%
set_mode('classification')
# workflow for RF
rf_wf <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 2)) %>%
add_recipe(rec_c)
# Fit the model
rf_fit <- fit(rf_wf, data = titanic_c)
# Get the OOB output
rf_OOB_output <- function(fit_model, truth) {
tibble(
.pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'),
#OOB predictions
class = truth
)
}
rf_output <- rf_OOB_output(rf_fit, titanic_c %>% pull(Survived))
rf_OOB <- rf_output %>%
accuracy(truth = class, estimate = .pred_class)
rf_output2 <- rf_wf %>%
update_model(rf_spec %>% set_args(importance = "permutation")) %>%
fit(data = titanic_c) %>%
extract_fit_engine()
rf_output2 %>%
vip(num_features = 30) + theme_classic()
rf_output2 %>% vip::vi() %>% head()
rf_output2 %>% vip::vi() %>% tail()
rf_confusion <- rf_OOB_output(rf_fit, titanic_c %>% pull(Survived)) %>%
conf_mat(truth = class, estimate = .pred_class)
rf_confusion
helper_confusion(rf_confusion)
knn_spec <-
nearest_neighbor() %>%
set_args(neighbors = tune()) %>%
set_engine(engine = 'kknn') %>%
set_mode('classification')
knn_wf <- workflow() %>%
add_model(knn_spec) %>%
add_recipe(rec_c)
penalty_grid <- grid_regular(neighbors(range = c(1, 100)),
levels = 20)
knn_cv <- tune_grid(knn_wf,
resamples = titanic_c_cv5,
grid = penalty_grid)
knn_cv %>%
collect_metrics() %>%
ggplot(aes(x = neighbors,
y = mean,
color = .metric)) +
geom_line()
knn_cv
knn_cv %>%
collect_metrics()
k <- knn_cv %>%
collect_metrics() %>%
filter(.metric == "accuracy") %>%
filter(mean == max(mean)) %>%
pull(neighbors)
# Process the data and make the recipe for classification
titanic_c <- Titanic_clean %>%
mutate(Survived = as.factor(Survived))
rec_c <- recipe(Survived ~ ., data = titanic_c) %>%
step_nzv(all_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_corr(all_numeric_predictors())
# Create k5 CV for the data
titanic_c_cv5 <- vfold_cv(titanic_c, v = 5)
# Helper method to calculate Sensitivity, Specificity, and Accuracy
helper_confusion <- function(x) {
return(list(
"Sensitivty" = x$table[1] / (x$table[1] + x$table[2]),
"Specificity" = x$table[4] / (x$table[3] + x$table[4]),
"Accuracy" = (x$table[1] + x$table[4]) / (x$table[1] + x$table[2] + x$table[3] + x$table[4])
))
}
# model spec for RF
rf_spec <- rand_forest() %>%
set_engine(engine = 'ranger') %>%
set_args(
mtry = NULL,
trees = 100,
min_n = 2,
probability = FALSE,
importance = 'impurity'
) %>%
set_mode('classification')
# workflow for RF
rf_wf <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 2)) %>%
add_recipe(rec_c)
# Fit the model
rf_fit <- fit(rf_wf, data = titanic_c)
# Get the OOB output
rf_OOB_output <- function(fit_model, truth) {
tibble(
.pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'),
#OOB predictions
class = truth
)
}
rf_output <- rf_OOB_output(rf_fit, titanic_c %>% pull(Survived))
rf_OOB <- rf_output %>%
accuracy(truth = class, estimate = .pred_class)
rf_output2 <- rf_wf %>%
update_model(rf_spec %>% set_args(importance = "permutation")) %>%
fit(data = titanic_c) %>%
extract_fit_engine()
rf_output2 %>%
vip(num_features = 30) + theme_classic()
rf_output2 %>% vip::vi() %>% head()
rf_output2 %>% vip::vi() %>% tail()
rf_confusion <- rf_OOB_output(rf_fit, titanic_c %>% pull(Survived)) %>%
conf_mat(truth = class, estimate = .pred_class)
rf_confusion
helper_confusion(rf_confusion)
knn_spec <-
nearest_neighbor() %>%
set_args(neighbors = tune()) %>%
set_engine(engine = 'kknn') %>%
set_mode('classification')
knn_wf <- workflow() %>%
add_model(knn_spec) %>%
add_recipe(rec_c)
penalty_grid <- grid_regular(neighbors(range = c(1, 100)),
levels = 20)
knn_cv <- tune_grid(knn_wf,
resamples = titanic_c_cv5,
grid = penalty_grid)
knn_cv %>%
collect_metrics() %>%
ggplot(aes(x = neighbors,
y = mean,
color = .metric)) +
geom_line()
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE, tidy = TRUE)
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(vip)
library(ranger)
library(rpart.plot)
library(broom)
library(kknn)
library(glmnet)
library(ISLR)
tidymodels_prefer()
theme_set(theme_bw())
set.seed(123)
data("Credit")
# Data Cleaning
Credit <- Credit %>%
na.omit() %>%
select(-ID, -Rating)
head(Credit)
# Create k5 CV for the data
Credit_cv5 <- vfold_cv(Credit, v = 5)
# Visualize the outcome variable
Credit %>%
ggplot(aes(x = Limit)) +
geom_histogram()
# Process the data and make the recipe for regression
rec_r <- recipe(Limit ~ ., data = Credit) %>%
step_nzv(all_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_corr(all_numeric_predictors())
# model spec for OLS
lm_spec <-
linear_reg() %>%
set_engine(engine = "lm") %>%
set_mode("regression")
# workflow for OLS
lm_wf <- workflow() %>%
add_recipe(rec_r) %>%
add_model(lm_spec)
# Fit the OLS model
lm_fit <- fit(lm_wf, Credit)
# Present the OLS model result and its metrics
tidy(lm_fit)
glance(lm_fit)
#  Calculate and collect CV metrics for OLS
lm_output <- fit_resamples(lm_wf,
resamples = Credit_cv5,
metrics = metric_set(rmse, rsq, mae))
collect_metrics(lm_output)
# Visualize the residual plot for OLS
lm_output2 <- lm_fit %>%
predict(new_data = Credit) %>%
bind_cols(Credit) %>%
mutate(resid = Limit - .pred)
ggplot(lm_output2, aes(x = .pred, y = resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept = 0, color = "red") +
theme_classic()
# model spec for LASSO
lasso_spec <-
linear_reg() %>%
set_args(mixture = 1, penalty = tune()) %>%
set_engine(engine = 'glmnet') %>%
set_mode('regression')
# workflows for LASSO
lasso_wf <- workflow() %>%
add_recipe(rec_r) %>%
add_model(lasso_spec)
# Fit the LASSO model
lasso_fit <- lasso_wf %>%
fit(data = Credit)
# Tune the LASSO model
glmnet_output <-
lasso_fit %>% extract_fit_parsnip() %>% pluck('fit')
lambdas <- glmnet_output$lambda
coefs_lambdas <-
coefficients(glmnet_output, s = lambdas)  %>%
as.matrix() %>%
t() %>%
as.data.frame() %>%
mutate(lambda = lambdas) %>%
select(lambda, everything(),-`(Intercept)`) %>%
pivot_longer(cols = -lambda,
names_to = "term",
values_to = "coef") %>%
mutate(var = map_chr(stringr::str_split(term, "_"),  ~ .[1]))
coefs_lambdas %>%
ggplot(aes(
x = lambda,
y = coef,
group = term,
color = var
)) +
geom_line() +
theme_classic() +
theme(legend.position = "bottom", legend.text = element_text(size = 8))
# Visualize the best penalty term
penalty_grid <- grid_regular(penalty(range = c(-3, 3)),
levels = 20)
lasso_cv <- tune_grid(
lasso_wf,
resamples = Credit_cv5,
metrics = metric_set(rmse, mae, rsq),
grid = penalty_grid
)
autoplot(lasso_cv) + theme_classic()
# Present the LASSO model result
best_penalty <- select_best(lasso_cv, metric = 'rmse')
lasso_wf2 <- finalize_workflow(lasso_wf, best_penalty)
lasso_fit <- fit(lasso_wf2, data = Credit)
tidy(lasso_fit)
#  Calculate and collect CV metrics for LASSO
lasso_output <- lasso_cv %>% collect_metrics(summarize = TRUE) %>%
na.omit() %>%
group_by(.metric) %>%
summarize(mean = mean(mean), std_err = mean(std_err))
lasso_output
# Visualize the residual plot for LASSO
lasso_output2 <- lasso_fit %>%
predict(new_data = Credit) %>%
bind_cols(Credit) %>%
mutate(resid = Limit - .pred)
ggplot(lasso_output2, aes(x = .pred, y = resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept = 0, color = "red") +
theme_classic()
# GAM model using mgcv
gam_spec <-
gen_additive_mod() %>%
set_engine(engine = 'mgcv') %>%
set_mode('regression')
gam_mod <- fit(gam_spec,
Limit ~ s(Income) + Cards + Age + Education + Gender + Student + Married + Ethnicity + s(Balance),
data = Credit)
# Present the GAM model result
gam_mod %>% pluck('fit') %>% summary()
# Compare the result with the linear GAMs model
gam_mod_temp <- fit(gam_spec,
Limit ~ Income + Cards + Age + Education + Gender + Student + Married + Ethnicity + Balance,
data = Credit)
gam_mod_temp %>% pluck('fit') %>% summary()
# Visualize the evaluation plots for GAMs
par(mfrow = c(2, 3))
gam_mod %>% pluck('fit') %>% mgcv::gam.check()
gam_mod %>% pluck('fit') %>% plot()
# Visualize the residual plot for GAMs
gam_output <- Credit %>%
bind_cols(predict(gam_mod, new_data = Credit)) %>%
mutate(resid = Limit - .pred)
ggplot(gam_output, aes(x = Income, y = resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept = 0, color = "red") +
theme_classic()
ggplot(gam_output, aes(x = Education, y = resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept = 0, color = "red") +
theme_classic()
ggplot(gam_output, aes(x = Balance, y = resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept = 0, color = "red") +
theme_classic()
ggplot(gam_output, aes(x = .pred, y = resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept = 0, color = "red") +
theme_classic()
#  Calculate and collect CV metrics for GAMs
gam_spec2 <-
linear_reg() %>%
set_engine(engine = 'lm') %>%
set_mode('regression')
gam_rec <- rec_r %>%
step_naomit(all_numeric_predictors(), skip = FALSE) %>%
step_ns(Income, deg_free = 1.481) %>%
step_ns(Balance, deg_free = 8.426)
gam_wf <- workflow() %>%
add_model(gam_spec2) %>%
add_recipe(gam_rec)
gam_output <- fit_resamples(gam_wf,
resamples = Credit_cv5,
metrics = metric_set(mae, rmse, rsq))
gam_output %>% collect_metrics()
regreDF <- lm_output %>%
collect_metrics() %>%
mutate(OLS = mean,
OLS_std = std_err) %>%
select(.metric, OLS, OLS_std) %>%
left_join(lasso_output %>%
mutate(LASSO = mean,
LASSO_std = std_err) %>%
select(.metric, LASSO, LASSO_std),
by = ".metric") %>%
left_join(gam_output %>%
collect_metrics() %>%
mutate(GAMs = mean,
GAMs_std = std_err) %>%
select(.metric, GAMs, GAMs_std),
by = ".metric")
regreDF
gam_mod %>% pluck('fit') %>% summary()
Pokemon_890 <- read_csv("Data/pokedex_(Update_05.20).csv") %>%
mutate(Name = name) %>%
select(-name)
Pokemon_721 <- read_csv("Data/Pokemon.csv")
# Data Cleaning
Pokemon_890 <- read_csv("Data/pokedex_(Update_05.20).csv") %>%
mutate(Name = name) %>%
select(-name)
Pokemon_721 <- read_csv("Data/Pokemon.csv")
Pokemon_final <- Pokemon_721 %>%
left_join(Pokemon_890,
by = "Name") %>%
select(c("#", Name, `Type 1`, `Type 2`, Total, HP, Attack, Defense, `Sp. Atk`, `Sp. Def`, Speed, Generation, Legendary, status, species, height_m, weight_kg, abilities_number, catch_rate, base_friendship, growth_rate)) %>%
distinct()
head(Pokemon_final)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE, tidy = TRUE)
library(tidyverse)
library(ggplot2)
library(dplyr)
set.seed(123)
Pokemon_890 <- read_csv("Data/pokedex_(Update_05.20).csv") %>%
mutate(Name = name) %>%
select(-name)
Pokemon_721 <- read_csv("Data/Pokemon.csv")
Pokemon_final <- Pokemon_721 %>%
left_join(Pokemon_890,
by = "Name") %>%
select(c("#", Name, `Type 1`, `Type 2`, Total, HP, Attack, Defense, `Sp. Atk`, `Sp. Def`, Speed, Generation, Legendary, status, species, height_m, weight_kg, abilities_number, catch_rate, base_friendship, growth_rate)) %>%
distinct()
Pokemon_final %>%
ggplot(aes(x = Attack,
y = `Sp. Atk`)) +
geom_point()
Pokemon_final %>%
ggplot(aes(x = Defense,
y = `Sp. Def`)) +
geom_point()
Pokemon_1 <- Pokemon_final %>%
select(Attack, `Sp. Atk`)
# Data-specific function to cluster and calculate total within-cluster SS
pokemon_cluster_ss <- function(k){
# Perform clustering
kclust <- kmeans(scale(Pokemon_1), centers = k)
# Return the total within-cluster sum of squares
return(kclust$tot.withinss)
}
tibble(
k = 1:15,
tot_wc_ss = purrr::map_dbl(1:15, pokemon_cluster_ss)
) %>%
ggplot(aes(x = k, y = tot_wc_ss)) +
geom_point() +
labs(x = "Number of clusters",y = 'Total within-cluster sum of squares') +
theme_classic()
tot_wc_ss
tibble(
k = 1:15,
tot_wc_ss = purrr::map_dbl(1:15, pokemon_cluster_ss)
)
