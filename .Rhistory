ggplot(aes(x = lambda, y = coef, group = term, color = var)) +
geom_line() +
theme_classic() +
theme(legend.position = "bottom", legend.text=element_text(size=8))
#  calculate/collect CV metrics
#  For linear regression
lm_result_5 <- fit_resamples(lm_wf,
resamples = Stock_clean_5,
metrics = metric_set(rmse, rsq, mae))
collect_metrics(lm_result_5)
#  for LASSO
penalty_grid <- grid_regular(
penalty(range = c(-3, 3)),
levels = 20)
tune_res <- tune_grid(
lm_lasso_wf,
resamples = Stock_clean_5,
metrics = metric_set(rmse, mae),
grid = penalty_grid
)
autoplot(tune_res) + theme_classic()
best_penalty <- select_best(tune_res, metric = 'rmse')
final_wf <- finalize_workflow(lm_lasso_wf, best_penalty)
final_fit <- fit(final_wf, data = Stock_clean)
tidy(final_fit)
tune_res %>% collect_metrics(summarize = TRUE) %>%
filter(.metric == "rmse") %>%
summarize(mean = mean(mean))
# recipes
data_rec <- recipe(Close ~ Date + Open + High + Low + rsi, data = Stock_clean) %>%
step_nzv(all_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_corr(all_numeric_predictors())
# workflows
lm_wf <- workflow() %>%
add_recipe(data_rec) %>%
add_model(lm_spec)
lm_lasso_wf <- workflow() %>%
add_recipe(data_rec) %>%
add_model(lm_lasso_spec)
# fit & tune models
fit_lm_model <- fit(lm_wf, Stock_clean)
tidy(fit_lm_model)
lm_lasso_fit <- lm_lasso_wf %>%
fit(data = Stock_clean)
glmnet_output <- lm_lasso_fit %>% extract_fit_parsnip() %>% pluck('fit')
lambdas <- glmnet_output$lambda
coefs_lambdas <-
coefficients(glmnet_output, s = lambdas )  %>%
as.matrix() %>%
t() %>%
as.data.frame() %>%
mutate(lambda = lambdas ) %>%
select(lambda, everything(), -`(Intercept)`) %>%
pivot_longer(cols = -lambda,
names_to = "term",
values_to = "coef") %>%
mutate(var = map_chr(stringr::str_split(term,"_"),~.[1]))
coefs_lambdas %>%
ggplot(aes(x = lambda, y = coef, group = term, color = var)) +
geom_line() +
theme_classic() +
theme(legend.position = "bottom", legend.text=element_text(size=8))
#  calculate/collect CV metrics
#  For linear regression
lm_result_5 <- fit_resamples(lm_wf,
resamples = Stock_clean_5,
metrics = metric_set(rmse, rsq, mae))
collect_metrics(lm_result_5)
#  for LASSO
penalty_grid <- grid_regular(
penalty(range = c(-3, 3)),
levels = 20)
tune_res <- tune_grid(
lm_lasso_wf,
resamples = Stock_clean_5,
metrics = metric_set(rmse, mae),
grid = penalty_grid
)
autoplot(tune_res) + theme_classic()
best_penalty <- select_best(tune_res, metric = 'rmse')
final_wf <- finalize_workflow(lm_lasso_wf, best_penalty)
final_fit <- fit(final_wf, data = Stock_clean)
tidy(final_fit)
tune_res %>% collect_metrics(summarize = TRUE) %>%
filter(.metric == "rmse") %>%
summarize(mean = mean(mean))
Stock_clean <- Stock_clean %>%
mutate(rsi = RSI(Close, n=14)) %>%
na.omit()
# creation of cv folds
Stock_clean_5 <- vfold_cv(Stock_clean, v = 5)
# model spec
lm_spec <-
linear_reg() %>%
set_engine(engine = "lm") %>%
set_mode("regression")
lm_lasso_spec <-
linear_reg() %>%
set_args(mixture = 1, penalty = tune()) %>%
set_engine(engine = 'glmnet') %>%
set_mode('regression')
# recipes
data_rec <- recipe(Close ~ Date + Open + High + Low + rsi, data = Stock_clean) %>%
step_nzv(all_predictors()) %>%
step_normalize(all_numeric_predictors())
#step_corr(all_numeric_predictors())
# workflows
lm_wf <- workflow() %>%
add_recipe(data_rec) %>%
add_model(lm_spec)
lm_lasso_wf <- workflow() %>%
add_recipe(data_rec) %>%
add_model(lm_lasso_spec)
# fit & tune models
fit_lm_model <- fit(lm_wf, Stock_clean)
tidy(fit_lm_model)
lm_lasso_fit <- lm_lasso_wf %>%
fit(data = Stock_clean)
glmnet_output <- lm_lasso_fit %>% extract_fit_parsnip() %>% pluck('fit')
lambdas <- glmnet_output$lambda
coefs_lambdas <-
coefficients(glmnet_output, s = lambdas )  %>%
as.matrix() %>%
t() %>%
as.data.frame() %>%
mutate(lambda = lambdas ) %>%
select(lambda, everything(), -`(Intercept)`) %>%
pivot_longer(cols = -lambda,
names_to = "term",
values_to = "coef") %>%
mutate(var = map_chr(stringr::str_split(term,"_"),~.[1]))
coefs_lambdas %>%
ggplot(aes(x = lambda, y = coef, group = term, color = var)) +
geom_line() +
theme_classic() +
theme(legend.position = "bottom", legend.text=element_text(size=8))
<<<<<<< HEAD
knitr::opts_chunk$set(echo = TRUE)
# library statements
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
tidymodels_prefer()
set.seed(123)
# read in data
Titanic <- read_csv("train.csv")
# data cleaning
Titanic_clean <- Titanic %>%
select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
na.omit()
Titanic_clean
# creation of cv folds
Titanic_clean_5 <- vfold_cv(Titanic_clean, v = 5)
# model spec
lm_spec <-
linear_reg() %>%
set_engine(engine = "lm") %>%
set_mode("regression")
lm_lasso_spec <-
linear_reg() %>%
set_args(mixture = 1, penalty = tune()) %>%
set_engine(engine = 'glmnet') %>%
set_mode('regression')
# recipes
data_rec <- recipe(Fare ~ ., data = Titanic_clean) %>%
step_nzv(all_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_corr(all_numeric_predictors())
# workflows
lm_wf <- workflow() %>%
add_recipe(data_rec) %>%
add_model(lm_spec)
lm_lasso_wf <- workflow() %>%
add_recipe(data_rec) %>%
add_model(lm_lasso_spec)
# fit & tune models
fit_lm_model <- fit(lm_wf, Titanic_clean)
tidy(fit_lm_model)
lm_lasso_fit <- lm_lasso_wf %>%
fit(data = Titanic_clean)
glmnet_output <- lm_lasso_fit %>% extract_fit_parsnip() %>% pluck('fit') # way to get the original glmnet output
lambdas <- glmnet_output$lambda
coefs_lambdas <-
coefficients(glmnet_output, s = lambdas )  %>%
as.matrix() %>%
t() %>%
as.data.frame() %>%
mutate(lambda = lambdas ) %>%
select(lambda, everything(), -`(Intercept)`) %>%
pivot_longer(cols = -lambda,
names_to = "term",
values_to = "coef") %>%
mutate(var = map_chr(stringr::str_split(term,"_"),~.[1]))
coefs_lambdas %>%
ggplot(aes(x = lambda, y = coef, group = term, color = var)) +
geom_line() +
theme_classic() +
theme(legend.position = "bottom", legend.text=element_text(size=8))
#  calculate/collect CV metrics
#  For linear regression
lm_result_5 <- fit_resamples(lm_wf,
resamples = Titanic_clean_5,
=======
#  calculate/collect CV metrics
#  For linear regression
lm_result_5 <- fit_resamples(lm_wf,
resamples = Stock_clean_5,
>>>>>>> 71981620e2ecb838d4512295c23e9263ea665b21
metrics = metric_set(rmse, rsq, mae))
collect_metrics(lm_result_5)
#  for LASSO
penalty_grid <- grid_regular(
penalty(range = c(-3, 3)),
levels = 20)
tune_res <- tune_grid(
lm_lasso_wf,
<<<<<<< HEAD
resamples = Titanic_clean_5,
=======
resamples = Stock_clean_5,
>>>>>>> 71981620e2ecb838d4512295c23e9263ea665b21
metrics = metric_set(rmse, mae),
grid = penalty_grid
)
autoplot(tune_res) + theme_classic()
best_penalty <- select_best(tune_res, metric = 'rmse')
final_wf <- finalize_workflow(lm_lasso_wf, best_penalty)
<<<<<<< HEAD
final_fit <- fit(final_wf, data = Titanic_clean)
=======
final_fit <- fit(final_wf, data = Stock_clean)
>>>>>>> 71981620e2ecb838d4512295c23e9263ea665b21
tidy(final_fit)
tune_res %>% collect_metrics(summarize = TRUE) %>%
filter(.metric == "rmse") %>%
summarize(mean = mean(mean))
<<<<<<< HEAD
#  visual residuals
lm_model_output <- fit_lm_model %>%
predict(new_data = Titanic_clean) %>%
bind_cols(Titanic_clean) %>%
mutate(resid = Fare - .pred)
ggplot(lm_model_output, aes(x = .pred, y = resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept = 0, color = "red") +
theme_classic()
lasso_model_output <- final_fit %>%
predict(new_data = Titanic_clean) %>%
bind_cols(Titanic_clean) %>%
mutate(resid = Fare - .pred)
ggplot(lasso_model_output, aes(x = .pred, y = resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept = 0, color = "red") +
theme_classic()
View(Titanic_clean)
gam_spec <-
gen_additive_mod() %>%
set_engine(engine = 'mgcv') %>%
set_mode('regression')
gam_mod <- fit(gam_spec,
Fare ~ Survived + Pclass + Sex + Age + SibSp + Parch + Embarked,
data = Titanic_clean
)
fit_gam_model %>% pluck('fit') %>% summary()
gam_model %>% pluck('fit') %>% summary()
gam_spec <-
gen_additive_mod() %>%
set_engine(engine = 'mgcv') %>%
set_mode('regression')
gam_mod <- fit(gam_spec,
Fare ~ Survived + Pclass + Sex + Age + SibSp + Parch + Embarked,
data = Titanic_clean
)
gam_mod %>% pluck('fit') %>% summary()
par(mfrow=c(2,2))
gam_mod %>% pluck('fit') %>% mgcv::gam.check()
gam_mod %>% pluck('fit') %>% plot()
gam_mod %>% pluck('fit') %>% summary()
gam_mod %>% pluck('fit') %>% plot()
set.seed(123)
gam_spec <-
gen_additive_mod() %>%
set_engine(engine = 'mgcv') %>%
set_mode('regression')
gam_mod <- fit(gam_spec,
Fare ~ Survived + Pclass + Sex + Age + SibSp + Parch + Embarked,
data = Titanic_clean
)
set.seed(123)
gam_spec <-
gen_additive_mod() %>%
set_engine(engine = 'mgcv') %>%
set_mode('regression')
gam_mod <- fit(gam_spec,
Fare ~ Survived + Pclass + Sex + Age + SibSp + Parch + Embarked,
data = Titanic_clean
)
set.seed(123)
gam_spec <-
gen_additive_mod() %>%
set_engine(engine = 'mgcv') %>%
set_mode('regression')
gam_mod <- fit(gam_spec,
Fare ~ Survived + Pclass + Sex + Age + SibSp + Parch + Embarked,
data = Titanic_clean
)
gam_mod %>% pluck('fit') %>% plot( all.terms = TRUE, pages = 1)
set.seed(123)
gam_spec <-
gen_additive_mod() %>%
set_engine(engine = 'mgcv') %>%
set_mode('regression')
gam_mod <- fit(gam_spec,
Fare ~ Survived + Pclass + Sex + Age + SibSp + Parch + Embarked,
data = Titanic_clean
)
gam_mod %>% pluck('fit') %>% plot()
set.seed(123)
gam_spec <-
gen_additive_mod() %>%
set_engine(engine = 'mgcv') %>%
set_mode('regression')
gam_mod <- fit(gam_spec,
Fare ~ Survived + Pclass + Sex + Age + s(SibSp) + Parch + Embarked,
data = Titanic_clean
)
set.seed(123)
gam_spec <-
gen_additive_mod() %>%
set_engine(engine = 'mgcv') %>%
set_mode('regression')
gam_mod <- fit(gam_spec,
Fare ~ Survived + s(Pclass) + Sex + Age + s(SibSp) + Parch + Embarked,
data = Titanic_clean
)
set.seed(123)
gam_spec <-
gen_additive_mod() %>%
set_engine(engine = 'mgcv') %>%
set_mode('regression')
gam_mod <- fit(gam_spec,
Fare ~ Survived + s(Pclass) + Sex + Age + SibSp + Parch + Embarked,
data = Titanic_clean
)
lm_spec <-
linear_reg() %>%
set_engine(engine = 'lm') %>%
set_mode('regression')
spline_rec <- data_rec %>%
step_ns(Pclass, deg_free = 10)
college_wf <- workflow() %>%
add_model(lm_spec) %>%
add_recipe(college_rec)
lm_spec <-
linear_reg() %>%
set_engine(engine = 'lm') %>%
set_mode('regression')
spline_rec <- data_rec %>%
step_ns(Pclass, deg_free = 10)
spline_wf <- workflow() %>%
add_model(lm_spec) %>%
add_recipe(spline_rec)
spline_fit <- spline_wf %>%
fit(data = Titanic_clean)
View(Titanic_clean)
Titanic_clean
spline_wf
lm_spec <-
linear_reg() %>%
set_engine(engine = 'lm') %>%
set_mode('regression')
spline_rec <- data_rec %>%
step_ns(Pclass, deg_free = 10)
spline_wf <- workflow() %>%
add_model(lm_spec) %>%
add_recipe(spline_rec)
spline_fit <- spline_wf %>%
fit(data = Titanic_clean)
spline_rec
lm_spec <-
linear_reg() %>%
set_engine(engine = 'lm') %>%
set_mode('regression')
spline_rec <- data_rec %>%
step_ns(Pclass, deg_free = 3)
spline_wf <- workflow() %>%
add_model(lm_spec) %>%
add_recipe(spline_rec)
spline_fit <- spline_wf %>%
fit(data = Titanic_clean)
Titanic_clean
fit_resamples(
spline_wf,
resamples = Titanic_clean_5,
metrics = metric_set(mae,rmse,rsq)
) %>% collect_metrics()
lm_spec <-
linear_reg() %>%
set_engine(engine = 'lm') %>%
set_mode('regression')
spline_rec <- data_rec %>%
step_ns(Pclass, deg_free = 3)
spline_wf <- workflow() %>%
add_model(lm_spec) %>%
add_recipe(spline_rec)
spline_fit <- spline_wf %>%
fit(data = Titanic_clean)
=======
>>>>>>> 71981620e2ecb838d4512295c23e9263ea665b21
knitr::opts_chunk$set(echo = TRUE)
# library statements
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
tidymodels_prefer()
library(vip)
library(ranger)
set.seed(123)
# read in data
Titanic <- read_csv("Data/train.csv")
# data cleaning
Titanic_clean <- Titanic %>%
select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
na.omit()
Titanic_clean
# creation of cv folds
Titanic_clean_5 <- vfold_cv(Titanic_clean, v = 5)
# recipes
data_rec <- recipe(Fare ~ ., data = Titanic_clean) %>%
step_nzv(all_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_corr(all_numeric_predictors())
set.seed(123)
# Model Specification
rf_spec <- rand_forest() %>%
set_engine(engine = 'ranger') %>%
set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors))
trees = 100, # Number of trees
min_n = 2,
probability = FALSE, # FALSE: get hard predictions (not needed for regression)
importance = 'impurity') %>% # we'll come back to this at the end
set_mode('classification') # change this for regression
Titanic_clean <-Titanic_clean %>%
mutate(Survived = as.factor(Survived))
titanic_rec <- recipe(Survived ~ ., data = Titanic_clean)
# Workflows
titanic_wf_2 <- workflow() %>%
add_model(rf_spec %>% set_args(mtry = 2)) %>%
add_recipe(titanic_rec)
set.seed(123) # make sure to run this before each fit so that you have the same 1000 trees
Titanic_fit <- fit(titanic_wf_2, data = Titanic_clean)
rf_OOB_output <- function(fit_model, truth){
tibble(
.pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
class = truth
)
}
#check out the function output
rf_output <- rf_OOB_output(Titanic_fit, Titanic_clean %>% pull(Survived))
data_rf_OOB_output <-  rf_output %>%
accuracy(truth = class, estimate = .pred_class)
titanic_output2 <- titanic_wf_2 %>%
update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
fit(data = Titanic_clean) %>%
extract_fit_engine()
titanic_output2 %>%
vip(num_features = 30) + theme_classic()
titanic_output2 %>% vip::vi() %>% head()
titanic_output2 %>% vip::vi() %>% tail()
rf_OOB_output(Titanic_fit, Titanic_clean %>% pull(Survived)) %>%
conf_mat(truth = class, estimate= .pred_class)
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = NULL,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth (max number of splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
titan_rec <- recipe(Survived ~ ., data = Titanic_clean) %>%
#step_unknown(all_nominal_predictors()) %>%
step_novel(all_nominal_predictors()) %>%
step_dummy(all_nominal_predictors())
titan_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(titan_rec)
titan_fit <- titan_wf %>%
fit(data = Titanic_clean)
titan_fit %>%
extract_fit_engine() %>%
rpart.plot()
# library statements
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
tidymodels_prefer()
library(vip)
library(ranger)
library(rpart.plot)
set.seed(123)
# read in data
Titanic <- read_csv("Data/train.csv")
titan_fit %>%
extract_fit_engine() %>%
rpart.plot()
