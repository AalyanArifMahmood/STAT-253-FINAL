---
title: "Final Project Code"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(vip)
library(ranger)

library(rpart.plot)
library(broom)
library(e1071)
library(caTools)
library(class)

tidymodels_prefer()
theme_set(theme_bw())


set.seed(123)

Titanic <- read_csv("Data/train.csv")
```

```{r}

Titanic_clean <- Titanic %>% 
  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>% 
    na.omit()

Titanic_clean

Titanic_clean_5 <- vfold_cv(Titanic_clean, v = 5)


data_rec <- recipe(Fare ~ ., data = Titanic_clean) %>%
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_corr(all_numeric_predictors())
```

# Classification

## RANDOM FOREST

```{r}
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = NULL,
           trees = 100, 
           min_n = 2,
           probability = FALSE,
           importance = 'impurity') %>% 
  set_mode('classification') 
```

```{r}
Titanic_clean <-Titanic_clean %>%
  mutate(Survived = as.factor(Survived))
```

```{r}
titanic_rec <- recipe(Survived ~ ., data = Titanic_clean)

titanic_wf_2 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 2)) %>%
  add_recipe(titanic_rec)
```

```{r}
set.seed(123)
Titanic_fit <- fit(titanic_wf_2, data = Titanic_clean)
```

```{r}
rf_OOB_output <- function(fit_model, truth){
    tibble(
          .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
          class = truth
      )
}

rf_output <- rf_OOB_output(Titanic_fit, Titanic_clean %>% pull(Survived))
```

```{r}
data_rf_OOB_output <-  rf_output %>% 
    accuracy(truth = class, estimate = .pred_class)
```


```{r}
titanic_output2 <- titanic_wf_2 %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% 
  fit(data = Titanic_clean) %>% 
    extract_fit_engine() 

titanic_output2 %>% 
    vip(num_features = 30) + theme_classic()


titanic_output2 %>% vip::vi() %>% head()
titanic_output2 %>% vip::vi() %>% tail()
```

```{r}
rf_OOB_output(Titanic_fit, Titanic_clean %>% pull(Survived)) %>%
    conf_mat(truth = class, estimate= .pred_class)
```

```{r}
#Accuracy:
(383+202)/(383+202+86+41)

#Specificity:
(202)/(202+86)

#Sensitivity:
(383)/(383+41)
```


### Discussion about Random Forest Classification:

> To evaluate the Random Forest model, we utilized OOB evaluation to avoid the computational intensity of Cross Validation in this case. Based on the results of the OOB evaluation, we output a confusion matrix to get the values of metrics such as accuracy, sensitivity and specificity.

> The goals of using Random Forest were to observe the variable importances of the many factors that could have affected the likelihood of survival of a particular passenger.

> Conclusion for RF: The model generally shows an acceptable performance with more than 82% accuracy. The model does a good job at predicting the correct/real outcome. We do not that the model does do relatively poorly at predicting those who survived, as the false negatives in our result are rather high (at 86), for which reason the specificity is relatively lower. This means that our model performs better at predicting the outcome of the passenger that did not survive, in comparison to a passenger who did survive.




## DECISION TREE


```{r}
ct_spec <- decision_tree() %>%
  set_engine(engine = 'rpart') %>%
  set_args(cost_complexity = NULL,
           min_n = NULL,
           tree_depth = NULL) %>%
  set_mode('classification') 
```

```{r}
titan_rec <- recipe(Survived ~ ., data = Titanic_clean) %>%
  step_novel(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

titan_wf <- workflow() %>%
  add_model(ct_spec) %>%
  add_recipe(titan_rec)
```

```{r}
titan_fit <- titan_wf %>%
  fit(data = Titanic_clean)
```

```{r}
titan_fit %>%
  extract_fit_engine() %>%
  rpart.plot()

dc_model_output <- titan_fit %>% 
  predict(new_data = Titanic_clean) %>%
  bind_cols(Titanic_clean)

dc_model_output %>% 
  conf_mat(truth = Survived, estimate= .pred_class)
```
### Evaluating Tree

```{r}
tree_output <-  bind_rows(
  predict(titan_fit, new_data = Titanic_clean) %>% bind_cols(Titanic_clean %>% select(Survived)) %>% mutate(model = 'orig'))

ct_metrics <- metric_set(sens, yardstick::spec, accuracy)

metrics <- tree_output %>% 
  group_by(model) %>%
  ct_metrics(estimate = .pred_class, truth = Survived) 


metrics %>% filter(.metric == 'accuracy') %>% arrange(desc(.estimate))
metrics %>% filter(.metric == 'sens') %>% arrange(desc(.estimate))
metrics %>% filter(.metric == 'spec') %>% arrange(desc(.estimate))
```

```{r}
metrics %>%
  ggplot(aes(x = .metric, y = .estimate, color = model)) +
  geom_point(size = 2) +
  geom_line(aes(group = model)) +
  theme_classic()
```


### Tuning Tree

```{r}
set.seed(123)
tree_fold <- vfold_cv(Titanic_clean, v=10)

titan_wf_tune <- titan_wf %>%
  update_model(ct_spec %>% set_args(cost_complexity = tune(), min_n = 5))

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 15) # try different values for levels

tune_res <- tune_grid(
  titan_wf_tune, 
  resamples = tree_fold, 
  grid = param_grid, 
  metrics = metric_set(accuracy, sens, yardstick::spec, roc_auc)
)

autoplot(tune_res) + theme_classic()

best_complexity <- select_best(tune_res, metric = 'accuracy')
titan_wf_final <- finalize_workflow(titan_wf_tune, best_complexity)

titan_final_fit <- fit(titan_wf_final, data = Titanic_clean) # fit final tuned model to training data

titan_final_fit %>% extract_fit_engine() %>% rpart.plot()

# CV Metrics for Tuned Classification Tree
tune_res %>% 
  collect_metrics() %>%
  filter(cost_complexity == best_complexity %>% pull(cost_complexity))
```



```{r}
#Accuracy:
(401+198)/(401+198+90+23)

#Specificity:
(198)/(198+90)

#Sensitivity:
(401)/(401+23)
```


## KNN

```{r}
# Model Specification
knn_spec <- 
  nearest_neighbor() %>%
  set_args(neighbors = tune()) %>% 
  set_engine(engine = 'kknn') %>%
  set_mode('classification') 

# Recipe
data_rec2 <- recipe(Survived ~ ., data = Titanic_clean) %>%
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_corr(all_numeric_predictors())

# Workflow (Recipe + Model)
knn_wf <- workflow() %>%
  add_model(knn_spec) %>% 
  add_recipe(data_rec2)
```

```{r}
Titanic_clean_52 <- vfold_cv(Titanic_clean, v = 5)

penalty_grid <- grid_regular(
  neighbors(range = c(1, 100)),
  levels = 20)

knn_fit_cv <- tune_grid(knn_wf,
                        resamples = Titanic_clean_52,
                        grid = penalty_grid)

knn_fit_cv %>% 
  collect_metrics() %>% 
  ggplot(aes(x = neighbors,
             y = mean,
             color = .metric)) +
  geom_line()
```

```{r}
k <- knn_fit_cv %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  filter(mean == max(mean)) %>% 
  pull(neighbors)

knn_spec2 <- 
  nearest_neighbor() %>%
  set_args(neighbors = k) %>% 
  set_engine(engine = 'kknn') %>%
  set_mode('classification')

knn_wf2 <- workflow() %>%
  add_model(knn_spec2) %>% 
  add_recipe(data_rec2)

fit_knn_model <- fit(knn_wf2, Titanic_clean)

knn_model_output <- fit_knn_model %>%
  predict(new_data = Titanic_clean) %>%
  bind_cols(Titanic_clean)

knn_model_output %>% 
  conf_mat(truth = Survived, estimate= .pred_class)
```

```{r}
#Accuracy:
(393+227)/(393+227+31+61)

#Specificity:
(227)/(227+61)

#Sensitivity:
(393)/(393+31)
```



# Regression

## OLS

```{r}
# model spec for OLS
lm_spec <-
  linear_reg() %>%
  set_engine(engine = "lm") %>%
  set_mode("regression")

# workflow for OLS
lm_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lm_spec) 
```

```{r}
# Fit the OLS model
fit_lm_model <- fit(lm_wf, Titanic_clean)
```

```{r}
# Present the OLS model result and its metrics
tidy(fit_lm_model)
glance(fit_lm_model)
```

```{r}
#  Calculate and collect CV metrics for OLS
lm_result_5 <- fit_resamples(lm_wf,
                             resamples = Titanic_clean_5,
                             metrics = metric_set(rmse, rsq, mae))

collect_metrics(lm_result_5)
```

```{r}
# Visualize the residual plot for OLS
lm_model_output <- fit_lm_model %>%
  predict(new_data = Titanic_clean) %>%
  bind_cols(Titanic_clean) %>%
  mutate(resid = Fare - .pred)

ggplot(lm_model_output, aes(x = .pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()
```

## LASSO

```{r}
# model spec for LASSO
lm_lasso_spec <-
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>%
  set_engine(engine = 'glmnet') %>%
  set_mode('regression') 

# workflows for LASSO
lm_lasso_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lm_lasso_spec)
```

```{r}
# Fit the LASSO model
lm_lasso_fit <- lm_lasso_wf %>%
  fit(data = Titanic_clean)
```

```{r}
# Tune the LASSO model
glmnet_output <-
  lm_lasso_fit %>% extract_fit_parsnip() %>% pluck('fit')

lambdas <- glmnet_output$lambda
coefs_lambdas <-
  coefficients(glmnet_output, s = lambdas)  %>%
  as.matrix() %>%
  t() %>%
  as.data.frame() %>%
  mutate(lambda = lambdas) %>%
  select(lambda, everything(),-`(Intercept)`) %>%
  pivot_longer(cols = -lambda,
               names_to = "term",
               values_to = "coef") %>%
  mutate(var = map_chr(stringr::str_split(term, "_"),  ~ .[1]))

coefs_lambdas %>%
  ggplot(aes(
    x = lambda,
    y = coef,
    group = term,
    color = var
  )) +
  geom_line() +
  theme_classic() +
  theme(legend.position = "bottom", legend.text = element_text(size = 8))
```

```{r}
# Visualize the best penalty term
penalty_grid <- grid_regular(penalty(range = c(-3, 3)),
                             levels = 20)

tune_res <- tune_grid(
  lm_lasso_wf,
  resamples = Titanic_clean_5,
  metrics = metric_set(rmse, mae, rsq),
  grid = penalty_grid
)

autoplot(tune_res) + theme_classic()
```

```{r}
# Present the LASSO model result
best_penalty <- select_best(tune_res, metric = 'rmse')
final_wf <- finalize_workflow(lm_lasso_wf, best_penalty)
final_fit <- fit(final_wf, data = Titanic_clean)
tidy(final_fit)
```

```{r}
#  Calculate and collect CV metrics for LASSO
lasso_result_5 <- tune_res %>% collect_metrics(summarize = TRUE) %>%
  na.omit() %>% 
  group_by(.metric) %>% 
  summarize(mean = mean(mean))
lasso_result_5
```

```{r}
# Visualize the residual plot for LASSO
lasso_model_output <- final_fit %>%
  predict(new_data = Titanic_clean) %>%
  bind_cols(Titanic_clean) %>%
  mutate(resid = Fare - .pred)

ggplot(lasso_model_output, aes(x = .pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()
```

## GAMs

```{r}
# GAM model using mgcv
gam_spec <-
  gen_additive_mod() %>%
  set_engine(engine = 'mgcv') %>%
  set_mode('regression')

gam_mod <- fit(gam_spec,
               Fare ~ Survived + Pclass + SibSp + Parch + Sex + Embarked + s(Age),
               data = Titanic_clean)
```

```{r}
# Present the GAM model result
gam_mod %>% pluck('fit') %>% summary() 
```

```{r}
# Visualize the evaluation plots for GAMs
par(mfrow = c(2, 3))
gam_mod %>% pluck('fit') %>% mgcv::gam.check()
gam_mod %>% pluck('fit') %>% plot()
```

```{r}
# Visualize the residual plot for GAMs
gam_mod_output <- Titanic_clean %>%
  bind_cols(predict(gam_mod, new_data = Titanic_clean)) %>%
  mutate(resid = Fare - .pred)

ggplot(gam_mod_output, aes(x = Age, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()

ggplot(gam_mod_output, aes(x = Parch, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()

ggplot(gam_mod_output, aes(x = .pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()
```
```{r}
#  Calculate and collect CV metrics for GAMs
spline_lm_spec <-
  linear_reg() %>%
  set_engine(engine = 'lm') %>%
  set_mode('regression')

spline_rec <- data_rec %>%
  step_naomit(all_numeric_predictors(), skip = FALSE) %>%
  step_ns(Age, deg_free = 2.694)

spline_wf <- workflow() %>%
  add_model(spline_lm_spec) %>%
  add_recipe(spline_rec)

gam_result_5 <- fit_resamples(spline_wf,
              resamples = Titanic_clean_5,
              metrics = metric_set(mae, rmse, rsq))

gam_result_5 %>% collect_metrics()
```

# Evaluation

```{r}
# CV metrics for OLS
lm_result_5 %>%
  collect_metrics()

# CV metrics for LASSO
lasso_result_5

# CV metrics for GAMs
gam_result_5 %>% 
  collect_metrics()
```

```{r}
# RF from the internet
library(ggplot2)
library(randomForest)
library(tidyverse)

set.seed(1)
train <- read.csv("data/train.csv", stringsAsFactors=FALSE)
test  <- read.csv("data/test.csv",  stringsAsFactors=FALSE)

extractFeatures <- function(data) {
  features <- c("Pclass",
                "Age",
                "Sex",
                "Parch",
                "SibSp",
                "Fare",
                "Embarked")
  fea <- data[,features]
  fea$Age[is.na(fea$Age)] <- -1
  fea$Fare[is.na(fea$Fare)] <- median(fea$Fare, na.rm=TRUE)
  fea$Embarked[fea$Embarked==""] = "S"
  fea$Sex      <- as.factor(fea$Sex)
  fea$Embarked <- as.factor(fea$Embarked)
  return(fea)
}

rf <- randomForest(extractFeatures(train), as.factor(train$Survived), ntree=100, importance=TRUE)

submission <- data.frame(PassengerId = test$PassengerId)
submission$Survived <- predict(rf, extractFeatures(test))

train <- train %>% 
  mutate(pred = predict(rf, extractFeatures(train))) %>% 
  select(pred, Survived)

imp <- importance(rf, type=1)
featureImportance <- data.frame(Feature=row.names(imp), Importance=imp[,1])

p <- ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +
     geom_bar(stat="identity", fill="#53cfff") +
     coord_flip() + 
     theme_light(base_size=20) +
     xlab("") +
     ylab("Importance") + 
     ggtitle("Random Forest Feature Importance\n") +
     theme(plot.title=element_text(size=18))

p

submission

train %>% 
  mutate(correct = ifelse(pred == Survived, "correct", "incorrect")) %>% 
  group_by(correct) %>% 
  summarize(sum = n())

rf
```