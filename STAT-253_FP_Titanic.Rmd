---
title: "Final Project Code"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# library statements 
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
tidymodels_prefer()
library(vip)
library(ranger)

set.seed(123)

# read in data
Titanic <- read_csv("Data/train.csv")
```

```{r}
# data cleaning
Titanic_clean <- Titanic %>% 
  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>% 
    na.omit()

Titanic_clean

# creation of cv folds
Titanic_clean_5 <- vfold_cv(Titanic_clean, v = 5)

# recipes
data_rec <- recipe(Fare ~ ., data = Titanic_clean) %>%
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_corr(all_numeric_predictors())
```

## Classification (RANDOM FOREST)

```{r}
set.seed(123)
# Model Specification
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors))
           trees = 100, # Number of trees
           min_n = 2,
           probability = FALSE, # FALSE: get hard predictions (not needed for regression)
           importance = 'impurity') %>% # we'll come back to this at the end
  set_mode('classification') # change this for regression
```

```{r}
Titanic_clean <-Titanic_clean %>%
  mutate(Survived = as.factor(Survived))
```


```{r}
titanic_rec <- recipe(Survived ~ ., data = Titanic_clean)

# Workflows
titanic_wf_2 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 2)) %>%
  add_recipe(titanic_rec)
```


```{r}
set.seed(123) # make sure to run this before each fit so that you have the same 1000 trees
Titanic_fit <- fit(titanic_wf_2, data = Titanic_clean)
```

```{r}
rf_OOB_output <- function(fit_model, truth){
    tibble(
          .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
          class = truth
      )
}

#check out the function output
rf_output <- rf_OOB_output(Titanic_fit, Titanic_clean %>% pull(Survived))
```

```{r}
data_rf_OOB_output <-  rf_output %>% 
    accuracy(truth = class, estimate = .pred_class)
```


```{r}
titanic_output2 <- titanic_wf_2 %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
  fit(data = Titanic_clean) %>% 
    extract_fit_engine() 

titanic_output2 %>% 
    vip(num_features = 30) + theme_classic()


titanic_output2 %>% vip::vi() %>% head()
titanic_output2 %>% vip::vi() %>% tail()
```

```{r}
rf_OOB_output(Titanic_fit, Titanic_clean %>% pull(Survived)) %>%
    conf_mat(truth = class, estimate= .pred_class)
```

## DECISION TREE

```{r}

```





### 1. Initial investigation: ignoring nonlinearity
#### For OLS

```{r}
# model spec for OLS
lm_spec <-
  linear_reg() %>%
  set_engine(engine = "lm") %>%
  set_mode("regression")

# workflow for OLS
lm_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lm_spec) 
```

```{r}
# Fit the OLS model
fit_lm_model <- fit(lm_wf, Titanic_clean)
```

```{r}
# Present the OLS model result and its metrics
tidy(fit_lm_model)
glance(fit_lm_model)
```

```{r}
#  Calculate and collect CV metrics for OLS
lm_result_5 <- fit_resamples(lm_wf,
                             resamples = Titanic_clean_5,
                             metrics = metric_set(rmse, rsq, mae))

collect_metrics(lm_result_5)
```

```{r}
# Visualize the residual plot for OLS
lm_model_output <- fit_lm_model %>%
  predict(new_data = Titanic_clean) %>%
  bind_cols(Titanic_clean) %>%
  mutate(resid = Fare - .pred)

ggplot(lm_model_output, aes(x = .pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()
```

#### For LASSO

```{r}
# model spec for LASSO
lm_lasso_spec <-
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>%
  set_engine(engine = 'glmnet') %>%
  set_mode('regression') 

# workflows for LASSO
lm_lasso_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lm_lasso_spec)
```

```{r}
# Fit the LASSO model
lm_lasso_fit <- lm_lasso_wf %>%
  fit(data = Titanic_clean)
```

```{r}
# Tune the LASSO model
glmnet_output <-
  lm_lasso_fit %>% extract_fit_parsnip() %>% pluck('fit')

lambdas <- glmnet_output$lambda
coefs_lambdas <-
  coefficients(glmnet_output, s = lambdas)  %>%
  as.matrix() %>%
  t() %>%
  as.data.frame() %>%
  mutate(lambda = lambdas) %>%
  select(lambda, everything(),-`(Intercept)`) %>%
  pivot_longer(cols = -lambda,
               names_to = "term",
               values_to = "coef") %>%
  mutate(var = map_chr(stringr::str_split(term, "_"),  ~ .[1]))

coefs_lambdas %>%
  ggplot(aes(
    x = lambda,
    y = coef,
    group = term,
    color = var
  )) +
  geom_line() +
  theme_classic() +
  theme(legend.position = "bottom", legend.text = element_text(size = 8))
```

```{r}
# Visualize the best penalty term
penalty_grid <- grid_regular(penalty(range = c(-3, 3)),
                             levels = 20)

tune_res <- tune_grid(
  lm_lasso_wf,
  resamples = Titanic_clean_5,
  metrics = metric_set(rmse, mae, rsq),
  grid = penalty_grid
)

autoplot(tune_res) + theme_classic()
```

```{r}
# Present the LASSO model result
best_penalty <- select_best(tune_res, metric = 'rmse')
final_wf <- finalize_workflow(lm_lasso_wf, best_penalty)
final_fit <- fit(final_wf, data = Titanic_clean)
tidy(final_fit)
```

```{r}
#  Calculate and collect CV metrics for LASSO
lasso_result_5 <- tune_res %>% collect_metrics(summarize = TRUE) %>%
  na.omit() %>% 
  group_by(.metric) %>% 
  summarize(mean = mean(mean))
lasso_result_5
```

```{r}
# Visualize the residual plot for LASSO
lasso_model_output <- final_fit %>%
  predict(new_data = Titanic_clean) %>%
  bind_cols(Titanic_clean) %>%
  mutate(resid = Fare - .pred)

ggplot(lasso_model_output, aes(x = .pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()
```

#### Evaluation (OLS and LASSO)

```{r}
# CV metrics for OLS
lm_result_5 %>% 
  collect_metrics()

# CV metrics for LASSO
lasso_result_5
```

The ticket class is obviously the most important variable, for the reason that for both OLS and LASSO, it has the smallest p-value and is the last variable come to zero when penalty increases. The outcome is similar to what we expect and the reality suggests, as people with higher ticket class definitely pays more for the ticket price.

By comparing the CV results for two models, we can see that the OLS model has lower MAE and RMSE and higher $R^2$ than the LASSO model, which indicates that we prefer OLS over LASSO to predict Fare for this dataset.

### 2. Accounting for nonlinearity

#### GAMs 

```{r}
# GAM model using mgcv
gam_spec <-
  gen_additive_mod() %>%
  set_engine(engine = 'mgcv') %>%
  set_mode('regression')

gam_mod <- fit(gam_spec,
               Fare ~ Survived + Pclass + SibSp + Parch + Sex + Embarked + s(Age),
               data = Titanic_clean)
```

```{r}
# Present the GAM model result
gam_mod %>% pluck('fit') %>% summary() 
```

```{r}
# Visualize the evaluation plots for GAMs
par(mfrow = c(2, 3))
gam_mod %>% pluck('fit') %>% mgcv::gam.check()
gam_mod %>% pluck('fit') %>% plot()
```

```{r}
# Visualize the residual plot for GAMs
gam_mod_output <- Titanic_clean %>%
  bind_cols(predict(gam_mod, new_data = Titanic_clean)) %>%
  mutate(resid = Fare - .pred)

ggplot(gam_mod_output, aes(x = Age, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()

ggplot(gam_mod_output, aes(x = Parch, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()

ggplot(gam_mod_output, aes(x = .pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()
```
```{r}
#  Calculate and collect CV metrics for GAMs
spline_lm_spec <-
  linear_reg() %>%
  set_engine(engine = 'lm') %>%
  set_mode('regression')

spline_rec <- data_rec %>%
  step_naomit(all_numeric_predictors(), skip = FALSE) %>%
  step_ns(Age, deg_free = 2.694)

spline_wf <- workflow() %>%
  add_model(spline_lm_spec) %>%
  add_recipe(spline_rec)

gam_result_5 <- fit_resamples(spline_wf,
              resamples = Titanic_clean_5,
              metrics = metric_set(mae, rmse, rsq))

gam_result_5 %>% collect_metrics()
```

#### Evaluation (GAMs)

From the GAM model result, considering the p-values for predictors, we can see that the important variables are Pclass, Parch, EmbarkedS, and SibSp, which correspond to those of LASSO and OLS models from the Investigation 1. However, the most relevant predictor does not change when accounting for nonlinearity, as it is a categorical variable.

From the GAM output plots for each predictor, we can see that Age does a great job as the blue and red lines overlap. However, Age is not an important variable in the model. Thus, the expected model performance between the GAM model and linear models would not change a lot.

```{r}
# CV metrics for OLS
lm_result_5 %>%
  collect_metrics()

# CV metrics for LASSO
lasso_result_5

# CV metrics for GAMs
gam_result_5 %>% 
  collect_metrics()
```

The MAE and RMSE values for the GAM model are lower than those of the OLS model and LASSO model, and the $R^2$ value for the GAM model is 0.4317564, which is slightly greater. This is reasonable as we make Age non-linear, but it is not an important predictor to the model and it would not largely affect the model result.


### 3. Summarize investigations 
We would go with the GAMs model, as it has lower RMSE and MAE and higher $R^2$. THe inerpretability of three models are all good. However, the predict accuracies of three models seem not to be reliable as the RMSE values for all models are quite high, considering the scale of the response variable.

### 4. Societal impact
They currently don't show any visible harms. Given the nature of the dataset, there seems to be no caution when we communicate our work.

```{r}
# RF from the internet
library(ggplot2)
library(randomForest)
library(tidyverse)

set.seed(1)
train <- read.csv("data/train.csv", stringsAsFactors=FALSE)
test  <- read.csv("data/test.csv",  stringsAsFactors=FALSE)

extractFeatures <- function(data) {
  features <- c("Pclass",
                "Age",
                "Sex",
                "Parch",
                "SibSp",
                "Fare",
                "Embarked")
  fea <- data[,features]
  fea$Age[is.na(fea$Age)] <- -1
  fea$Fare[is.na(fea$Fare)] <- median(fea$Fare, na.rm=TRUE)
  fea$Embarked[fea$Embarked==""] = "S"
  fea$Sex      <- as.factor(fea$Sex)
  fea$Embarked <- as.factor(fea$Embarked)
  return(fea)
}

rf <- randomForest(extractFeatures(train), as.factor(train$Survived), ntree=100, importance=TRUE)

submission <- data.frame(PassengerId = test$PassengerId)
submission$Survived <- predict(rf, extractFeatures(test))

train <- train %>% 
  mutate(pred = predict(rf, extractFeatures(train))) %>% 
  select(pred, Survived)

imp <- importance(rf, type=1)
featureImportance <- data.frame(Feature=row.names(imp), Importance=imp[,1])

p <- ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +
     geom_bar(stat="identity", fill="#53cfff") +
     coord_flip() + 
     theme_light(base_size=20) +
     xlab("") +
     ylab("Importance") + 
     ggtitle("Random Forest Feature Importance\n") +
     theme(plot.title=element_text(size=18))

p

submission

train %>% 
  mutate(correct = ifelse(pred == Survived, "correct", "incorrect")) %>% 
  group_by(correct) %>% 
  summarize(sum = n())

rf
```