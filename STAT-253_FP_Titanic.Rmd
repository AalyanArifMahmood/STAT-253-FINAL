---
title: "Final Project Code"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 1. Initial investigation: ignoring nonlinearity

#### a & b.

```{r}
# library statements 
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
tidymodels_prefer()

set.seed(123)

# read in data
Titanic <- read_csv("Data/train.csv")
```

```{r}
# data cleaning
Titanic_clean <- Titanic %>% 
  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>% 
    na.omit()

Titanic_clean
```

```{r}
# creation of cv folds
Titanic_clean_5 <- vfold_cv(Titanic_clean, v = 5)
```

```{r}
# model spec
lm_spec <-
    linear_reg() %>%
    set_engine(engine = "lm") %>% 
    set_mode("regression")

lm_lasso_spec <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>%
  set_engine(engine = 'glmnet') %>%
  set_mode('regression') 
```

```{r}
# recipes
data_rec <- recipe(Fare ~ ., data = Titanic_clean) %>%
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_corr(all_numeric_predictors())

# workflows
lm_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lm_spec) 

lm_lasso_wf <- workflow() %>% 
  add_recipe(data_rec) %>%
  add_model(lm_lasso_spec)
```

```{r}
# fit & tune models
fit_lm_model <- fit(lm_wf, Titanic_clean)
tidy(fit_lm_model)

lm_lasso_fit <- lm_lasso_wf %>% 
  fit(data = Titanic_clean)

glmnet_output <- lm_lasso_fit %>% extract_fit_parsnip() %>% pluck('fit') # way to get the original glmnet output

lambdas <- glmnet_output$lambda
coefs_lambdas <- 
  coefficients(glmnet_output, s = lambdas )  %>% 
  as.matrix() %>%  
  t() %>% 
  as.data.frame() %>% 
  mutate(lambda = lambdas ) %>% 
  select(lambda, everything(), -`(Intercept)`) %>% 
  pivot_longer(cols = -lambda, 
               names_to = "term", 
               values_to = "coef") %>%
  mutate(var = map_chr(stringr::str_split(term,"_"),~.[1]))

coefs_lambdas %>%
  ggplot(aes(x = lambda, y = coef, group = term, color = var)) +
  geom_line() +
  theme_classic() + 
  theme(legend.position = "bottom", legend.text=element_text(size=8))
```

#### c.

```{r}
#  calculate/collect CV metrics
#  For linear regression
lm_result_5 <- fit_resamples(lm_wf,
                             resamples = Titanic_clean_5, 
                             metrics = metric_set(rmse, rsq, mae))

collect_metrics(lm_result_5)


#  for LASSO
penalty_grid <- grid_regular(
  penalty(range = c(-3, 3)),
  levels = 20)

tune_res <- tune_grid(
  lm_lasso_wf, 
  resamples = Titanic_clean_5,
  metrics = metric_set(rmse, mae),
  grid = penalty_grid 
)

autoplot(tune_res) + theme_classic()

best_penalty <- select_best(tune_res, metric = 'rmse')

final_wf <- finalize_workflow(lm_lasso_wf, best_penalty)

final_fit <- fit(final_wf, data = Titanic_clean)

tidy(final_fit)

tune_res %>% collect_metrics(summarize = TRUE) %>% 
  filter(.metric == "rmse") %>% 
  summarize(mean = mean(mean))
```

#### d.

```{r}
#  visual residuals
lm_model_output <- fit_lm_model %>% 
    predict(new_data = Titanic_clean) %>%
    bind_cols(Titanic_clean) %>%
    mutate(resid = Fare - .pred)

ggplot(lm_model_output, aes(x = .pred, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

lasso_model_output <- final_fit %>% 
    predict(new_data = Titanic_clean) %>%
    bind_cols(Titanic_clean) %>%
    mutate(resid = Fare - .pred)

ggplot(lasso_model_output, aes(x = .pred, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
```

#### e. 
The ticket class is obviously the most important variable, for the reason that for both OLS and LASSO, it has the smallest p-value and is the last variable come to zero when penalty increases. The outcome is similar to what we expect and the reality suggests, as people with higher ticket class definitely pays more for the ticket price.

### 2. Accounting for nonlinearity

```{r}
lm_spec <-
  linear_reg() %>%
  set_engine(engine = 'lm') %>%
  set_mode('regression')
data_rec <- recipe(Fare ~ Survived + Pclass + Sex + Age + SibSp + Parch + Embarked, data = Titanic_clean) %>%
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_corr(all_numeric_predictors())

spline_rec <- data_rec %>%
  step_naomit(all_numeric_predictors(), skip = FALSE) %>%
  step_ns(Age, deg_free = 5)

spline_wf <- workflow() %>%
    add_model(lm_spec) %>%
    add_recipe(spline_rec)

spline_fit <- spline_wf %>% 
  fit(data = Titanic_clean)

# fit_resamples(
#     spline_wf,
#     resamples = Titanic_clean_5,
#     metrics = metric_set(mae,rmse,rsq)                     
# ) %>% collect_metrics()
tidy(spline_fit)
```
```{r}
spline_mod_output <- Titanic_clean %>%
  bind_cols(predict(spline_fit, new_data = Titanic_clean)) %>%
    mutate(resid = Fare - .pred)

# Residual plots
ggplot(spline_mod_output, aes(x = Age, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(spline_mod_output, aes(x = Parch, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(spline_mod_output, aes(x = .pred, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()


```

### 3. Summarize investigations 
We would go with the orinary least square model, as it has lower RMSE than the LASSO model. The predict accuracies of two models seem not to be reliable as the RMSE values for both models are quite high, considering the scale of the response variable.

### 4. Societal impact
They currently don't show any visible harms. Given the nature of the dataset, there seems to be no caution when we communicate our work.
