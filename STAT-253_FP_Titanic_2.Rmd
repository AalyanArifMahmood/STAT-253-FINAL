---
title: "STAT-253_FP_temp"
author: "Aalyan, Eric"
date: "4/7/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE, tidy = TRUE)
```

```{r}
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(vip)
library(ranger)
library(rpart.plot)
library(broom)
library(kknn)
library(glmnet)

tidymodels_prefer()
theme_set(theme_bw())

set.seed(123)

Titanic <- read_csv("Data/train.csv")
```

```{r}
# Data Cleaning
Titanic_clean <- Titanic %>% 
  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>% 
    na.omit()

Titanic_clean
```

## Classification

```{r}
# Process the data and make the recipe for classification
titanic_c <- Titanic_clean %>%
  mutate(Survived = as.factor(Survived))

rec_c <- recipe(Survived ~ ., data = titanic_c) %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(all_numeric_predictors())

titanic_c_cv5 <- vfold_cv(titanic_c, v = 5)
```

```{r}
# Helper method to calculate Sens, Spec, and Accu
helper_confusion <- function(x) {
  return(list(
    "Sensitivty" = x$table[1] / (x$table[1] + x$table[2]),
    "Specificity" = x$table[4] / (x$table[3] + x$table[4]),
    "Accuracy" = (x$table[1] + x$table[4]) / (x$table[1] + x$table[2] + x$table[3] + x$table[4])
  ))
}
```

### Random Forest

```{r}
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>%
  set_args(
    mtry = NULL,
    trees = 100,
    min_n = 2,
    probability = FALSE,
    importance = 'impurity'
  ) %>%
  set_mode('classification')

rf_wf <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 2)) %>%
  add_recipe(rec_c)
```

```{r}
rf_fit <- fit(rf_wf, data = titanic_c)
```

```{r}
rf_OOB_output <- function(fit_model, truth) {
  tibble(
    .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'),
    #OOB predictions
    class = truth
  )
}

rf_output <- rf_OOB_output(rf_fit, titanic_c %>% pull(Survived))

rf_OOB <- rf_output %>%
  accuracy(truth = class, estimate = .pred_class)
```

```{r}
rf_output2 <- rf_wf %>%
  update_model(rf_spec %>% set_args(importance = "permutation")) %>%
  fit(data = titanic_c) %>%
  extract_fit_engine()

rf_output2 %>%
  vip(num_features = 30) + theme_classic()

rf_output2 %>% vip::vi() %>% head()
rf_output2 %>% vip::vi() %>% tail()
```

```{r}
rf_confusion <- rf_OOB_output(rf_fit, titanic_c %>% pull(Survived)) %>%
  conf_mat(truth = class, estimate = .pred_class)
rf_confusion
```

```{r}
helper_confusion(rf_confusion)
```


## Discussion about Random Forest Classification:

> To evaluate the Random Forest model, we utilized OOB evaluation to avoid the computational
intensity of Cross Validation in this case. Based on the results of the OOB evaluation, we output
a confusion matrix to get the values of metrics such as accuracy, sensitivity and specificity.
The goals of using Random Forest were to observe the variable importances of the many factors
that could have affected the likelihood of survival of a particular passenger.
Conclusion for RF: The model generally shows an acceptable performance with more than 82%
accuracy. The model does a good job at predicting the correct/real outcome. We do not that
the model does do relatively poorly at predicting those who survived, as the false negatives in
our result are rather high (at 86), for which reason the specificity is relatively lower. This means
that our model performs better at predicting the outcome of the passenger that did not survive,
in comparison to a passenger who did survive.


### Decision Tree

```{r}
dt_spec <- decision_tree() %>%
  set_engine(engine = 'rpart') %>%
  set_args(cost_complexity = NULL,
           min_n = NULL,
           tree_depth = NULL) %>%
  set_mode('classification')

dt_wf <- workflow() %>%
  add_model(dt_spec) %>%
  add_recipe(rec_c)
```

```{r}
dt_fit <- dt_wf %>% fit(data = titanic_c)
```

```{r}
dt_fit %>%
  extract_fit_engine() %>%
  rpart.plot()

dt_output <- dt_fit %>%
  predict(new_data = titanic_c) %>%
  bind_cols(titanic_c)

dt_confusion <- dt_output %>%
  conf_mat(truth = Survived, estimate = .pred_class)

dt_confusion
```

```{r}
helper_confusion(dt_confusion)
```

### KNN

```{r}
knn_spec <- 
  nearest_neighbor() %>%
  set_args(neighbors = tune()) %>% 
  set_engine(engine = 'kknn') %>%
  set_mode('classification') 

knn_wf <- workflow() %>%
  add_model(knn_spec) %>% 
  add_recipe(rec_c)
```

```{r}
penalty_grid <- grid_regular(neighbors(range = c(1, 100)),
                             levels = 20)

knn_cv <- tune_grid(knn_wf,
                    resamples = titanic_c_cv5,
                    grid = penalty_grid)

knn_cv %>%
  collect_metrics() %>%
  ggplot(aes(x = neighbors,
             y = mean,
             color = .metric)) +
  geom_line()
```

```{r}
k <- knn_cv %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  filter(mean == max(mean)) %>% 
  pull(neighbors)

knn_spec2 <- 
  nearest_neighbor() %>%
  set_args(neighbors = k) %>% 
  set_engine(engine = 'kknn') %>%
  set_mode('classification')

knn_wf2 <- workflow() %>%
  add_model(knn_spec2) %>% 
  add_recipe(rec_c)

knn_fit <- fit(knn_wf2, titanic_c)

knn_output <- knn_fit %>%
  predict(new_data = titanic_c) %>%
  bind_cols(titanic_c)

knn_confusion <- knn_output %>% 
  conf_mat(truth = Survived, estimate= .pred_class)

helper_confusion(knn_confusion)
```

### Evaluation (RF, DT, KNN)

```{r}
cbind(RF = helper_confusion(rf_confusion)) %>% 
  cbind(DT = helper_confusion(dt_confusion)) %>% 
  cbind(KNN = helper_confusion(knn_confusion))
```


> To evaluate the KNN model, we utilized CV to select the best tuning parameters. Based on the
results shown in the graph above, we select k = 21 for the number of neighbors. Then, we get
the prediction based on KNN model for k = 21, and calculate the metrics.
Conclusion for KNN: The model generally shows an acceptable performance with more than
87% accuracy. The model does a good job at predicting the correct/real outcome. We do not
that the model does do relatively poorly at predicting those who survived, , for which reason
the specificity is relatively lower. This means that, similar to RF, our model performs better at
predicting the outcome of the passenger that did not survive, in comparison to a passenger who
did survive.

## Comparison between KNN Classification and Random Forest Classification

> Using accuracy (cross validated in case of KNN and OOB in case of RF) as our metric (since in this
context, sensitivity and specificity are equally important), we see that KNN classification performs
better than Random forest as it yields a larger accuracy than RF. However, it is important to
note that KNN Classification is more prone to overfitting than RF, and should we fit both models
to unseen data, we may observe superior performance from RF