---
title: "STAT-253_Final"
author: "Aalyan Mahmood, Eric Wang"
date: "4/26/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE, tidy = TRUE)
```

```{r}
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(vip)
library(ranger)
library(rpart.plot)
library(broom)
library(kknn)
library(glmnet)
library(ISLR)

tidymodels_prefer()
theme_set(theme_bw())

set.seed(123)
```

## Part 1: Regression

### 1 Data Context

```{r}
data("Credit")

# Data Cleaning
Credit <- Credit %>% 
  na.omit() %>% 
  select(-ID, -Rating)
head(Credit)

# Create k5 CV for the data
Credit_cv5 <- vfold_cv(Credit, v = 5)
```

For the regression part, we use a simulated data set containing information on 400 customers. In this dataset:

+ `Income`: the individual's income in $1,000's
+ `Limit`: the credit limit
+ `Cards`: the number of credit cards
+ `Age`: the age in years
+ `Education`: the number of years of eduction
+ `Gender`: a factor with levels Male and Female
+ `Student`: a factor with levels No and Yes indicating whether the individual was a student
+ `Married`: a factor with levels No and Yes indicating whether the individual was married
+ `Ethnicity`: a factor with levels African American, Asian, and Caucasian indicating the individual's ethnicity
+ `Balance`: the average credit card balance in dollar
  
The dataset is simulated data, with thanks to Albert Kim for pointing out that this was omitted, and supplying the data and man documentation page on Oct 19, 2017.

### 2 Research Questions

We fit regression models to predict individual's credit limit in dollar. The graph below is the distribution of `Limit`.

```{r}
# Visualize the outcome variable
Credit %>% 
  ggplot(aes(x = Limit)) + 
  geom_histogram()
```

### 3 Methods

```{r}
# Process the data and make the recipe for regression
rec_r <- recipe(Limit ~ ., data = Credit) %>%
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_corr(all_numeric_predictors())
```

#### 3.1 OLS

```{r}
# model spec for OLS
lm_spec <-
  linear_reg() %>%
  set_engine(engine = "lm") %>%
  set_mode("regression")

# workflow for OLS
lm_wf <- workflow() %>%
  add_recipe(rec_r) %>%
  add_model(lm_spec) 
```

```{r}
# Fit the OLS model
lm_fit <- fit(lm_wf, Credit)
```

```{r}
# Present the OLS model result and its metrics
tidy(lm_fit)
glance(lm_fit)
```

```{r}
#  Calculate and collect CV metrics for OLS
lm_output <- fit_resamples(lm_wf,
                           resamples = Credit_cv5,
                           metrics = metric_set(rmse, rsq, mae))

collect_metrics(lm_output)
```

```{r}
# Visualize the residual plot for OLS
lm_output2 <- lm_fit %>%
  predict(new_data = Credit) %>%
  bind_cols(Credit) %>%
  mutate(resid = Limit - .pred)

ggplot(lm_output2, aes(x = .pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()
```

The first model we use is ordinary least square (linear regression). 

Based on 5-folds cross-validation, we get MAE(272.707), RMSE(365.188), $R^2$(0.975). Overall, as the range of the outcome value `Limit` is quite large (over 10000), MAE and RMSE for this model are not very large. Also, the $R^2$ indicates that the model has a good performance in predicting the outcome. 

We also use a residual plot to visualize the prediction. However, we can see that points are not randomly positioned on the plot, and there is still a significant difference between the red line and blue line. The residual plot shows that there the model is not good enough in general.

#### 3.2 LASSO

```{r}
# model spec for LASSO
lasso_spec <-
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>%
  set_engine(engine = 'glmnet') %>%
  set_mode('regression') 

# workflows for LASSO
lasso_wf <- workflow() %>%
  add_recipe(rec_r) %>%
  add_model(lasso_spec)
```

```{r}
# Fit the LASSO model
lasso_fit <- lasso_wf %>%
  fit(data = Credit)
```

```{r}
# Tune the LASSO model
glmnet_output <-
  lasso_fit %>% extract_fit_parsnip() %>% pluck('fit')

lambdas <- glmnet_output$lambda

coefs_lambdas <-
  coefficients(glmnet_output, s = lambdas)  %>%
  as.matrix() %>%
  t() %>%
  as.data.frame() %>%
  mutate(lambda = lambdas) %>%
  select(lambda, everything(),-`(Intercept)`) %>%
  pivot_longer(cols = -lambda,
               names_to = "term",
               values_to = "coef") %>%
  mutate(var = map_chr(stringr::str_split(term, "_"),  ~ .[1]))

coefs_lambdas %>%
  ggplot(aes(
    x = lambda,
    y = coef,
    group = term,
    color = var
  )) +
  geom_line() +
  theme_classic() +
  theme(legend.position = "bottom", legend.text = element_text(size = 8))
```

```{r}
# Visualize the best penalty term
penalty_grid <- grid_regular(penalty(range = c(-3, 3)),
                             levels = 20)

lasso_cv <- tune_grid(
  lasso_wf,
  resamples = Credit_cv5,
  metrics = metric_set(rmse, mae, rsq),
  grid = penalty_grid
)

autoplot(lasso_cv) + theme_classic()
```

```{r}
# Present the LASSO model result
best_penalty <- select_best(lasso_cv, metric = 'rmse')
lasso_wf2 <- finalize_workflow(lasso_wf, best_penalty)
lasso_fit <- fit(lasso_wf2, data = Credit)
tidy(lasso_fit)
```

```{r}
#  Calculate and collect CV metrics for LASSO
lasso_output <- lasso_cv %>% collect_metrics(summarize = TRUE) %>%
  na.omit() %>% 
  group_by(.metric) %>% 
  summarize(mean = mean(mean), std_err = mean(std_err))
lasso_output
```

```{r}
# Visualize the residual plot for LASSO
lasso_output2 <- lasso_fit %>%
  predict(new_data = Credit) %>%
  bind_cols(Credit) %>%
  mutate(resid = Limit - .pred)

ggplot(lasso_output2, aes(x = .pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()
```

The next method we use is LASSO. 

We plot a path graph for all the parameters. The longer it takes for the variable to become zero, the more important it is in the model. Thus, the graph shows that the most important variable is `Balance`, and the least important variables are `Gender` and `Ethnicity`. 

Next, we plot three graphs with the amount of regularization on the x-axis and statistics metrics on the y-axis. We then select the penalty number based on the best RMSE and fit the final model. We evaluate the model with MAE(337.641), RMSE(446.247), and $R^2$(0.970). In general, MAE and RMSE are not large, and $R^2$ is quite high.

We also make a residual plot to evaluate the model. Same to OLS, points are not randomly scattered around, and there is still a large difference between the blue and red lines.

#### 3.3 GAMs

```{r}
# GAM model using mgcv
gam_spec <-
  gen_additive_mod() %>%
  set_engine(engine = 'mgcv') %>%
  set_mode('regression')

gam_mod <- fit(gam_spec,
               Limit ~ s(Income) + Cards + Age + Education + Gender + Student + Married + Ethnicity + s(Balance),
               data = Credit)
```

```{r}
# Present the GAM model result
gam_mod %>% pluck('fit') %>% summary() 
```

```{r}
# Compare the result with the linear GAMs model
gam_mod_temp <- fit(gam_spec,
               Limit ~ Income + Cards + Age + Education + Gender + Student + Married + Ethnicity + Balance,
               data = Credit)
gam_mod_temp %>% pluck('fit') %>% summary() 
```

```{r}
# Visualize the evaluation plots for GAMs
par(mfrow = c(2, 3))
gam_mod %>% pluck('fit') %>% mgcv::gam.check()
gam_mod %>% pluck('fit') %>% plot()
```

```{r}
# Visualize the residual plot for GAMs
gam_output <- Credit %>%
  bind_cols(predict(gam_mod, new_data = Credit)) %>%
  mutate(resid = Limit - .pred)

ggplot(gam_output, aes(x = Income, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()

ggplot(gam_output, aes(x = Education, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()

ggplot(gam_output, aes(x = Balance, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()

ggplot(gam_output, aes(x = .pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()
```

```{r}
#  Calculate and collect CV metrics for GAMs
gam_spec2 <-
  linear_reg() %>%
  set_engine(engine = 'lm') %>%
  set_mode('regression')

gam_rec <- rec_r %>%
  step_naomit(all_numeric_predictors(), skip = FALSE) %>%
  step_ns(Income, deg_free = 1.481) %>% 
  step_ns(Balance, deg_free = 8.426)

gam_wf <- workflow() %>%
  add_model(gam_spec2) %>%
  add_recipe(gam_rec)

gam_output <- fit_resamples(gam_wf,
                            resamples = Credit_cv5,
                            metrics = metric_set(mae, rmse, rsq))

gam_output %>% collect_metrics()
```

The third model we use is GAMs. 

There are five numerical variables in the model: `Cards`, `Age`, `Education`, `Income`, and `Balance`, but the first three variables are essentially categorical variables, which are only expressed in numerical form. Therefore, we add splines to `Income`, `Education`, and `Balance`. Compared to the linear GAMs model, we can see the $R^2$ and deviance explained both increase, which means the added splines are effective.

We use "mgcv" engine to find the best edf and plot several graphs to see the results. We can see from those plots that GAMs work quite well. Especially for the last one, the residual plot, the blue and red lines basically overlap. It indicates that the model fits well. Based on the variable importance shown by the GAMs model output, `Cards` and `Student` are the most important variables, while `Married`, `Gender`, and `Ethnicity` are the least important variables.

From the perspective of metrics, MAE(149.967), RMSE(249.692), and $R^2$(0.988) show that the model has a good performance and compared to the previous two methods, it has a quite large improvement. 

### 4 Evaluations

```{r}
regreDF <- lm_output %>%
  collect_metrics() %>% 
  mutate(OLS = mean,
         OLS_std = std_err) %>% 
  select(.metric, OLS, OLS_std) %>% 
  left_join(lasso_output %>% 
              mutate(LASSO = mean,
                     LASSO_std = std_err) %>% 
              select(.metric, LASSO, LASSO_std), 
            by = ".metric") %>% 
  left_join(gam_output %>% 
              collect_metrics() %>% 
              mutate(GAMs = mean,
                     GAMs_std = std_err) %>% 
              select(.metric, GAMs, GAMs_std), 
            by = ".metric")

regreDF
```

For the evaluation metrics based on 5-folds cross-validation, LASSO has the highest MAE and RMSE and the lowest $R^2$, and GAMs achieve the lowest MAE and RMSE and the highest $R^2$. The MAEs and RMSEs in this situation are quite large, as the size of the outcome variable `Limit`, and these large metrics indicate that our outcomes have a large uncertainty and could lead to unfair credit card limits.

The residual plots for OLS and LASSO are quite similar, while that for GAMs shows a better performance, as the blue line and red line overlap more in the residual plot of the GAMs model. 

From the perspective of variable importance, the importance sequence (from high to low) for LASSO is `Balance`, `Income`, `Student`, and `Cards`, while `s(Balance)`, `s(Income)`, and `StudentYes` have the same variable importance in the GAMs model. Both the two sequences are reasonable intuitively. In reality, balance and income are two important indicators for credit card limits. Also, if a person is a student, that person is less able to repay the money, which means he or she is more likely to get a lower credit card limit.

Looking at the residual plots of all the models, there does seem to be some systematic bias because most of the residuals are focused in the the region below 6000 in the residual plots for all of our models. Given that the majority of the cases in our dataset are not students, this indicates a rather low income standing in the majority of the cases. 

Thus, the final model we choose is the GAMs model.

### 5 Conclusions

```{r}
gam_mod %>% pluck('fit') %>% summary() 
```

**Interpret you final model (show plots of estimated non-linear functions, or slope coefficients) for important predictors, and provide some general interpretations of what you learn from these: negative or positive coefficients?**
**Interpret evaluation metric(s) for the final model in context with units. Does the model show an acceptable amount of error?: meaning of MAE, RMSE, RSQ**
**Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results. Sensibility??**

## Part 2: Classification

### 1 Data Context

```{r}
Titanic <- read_csv("Data/train.csv")

# Data Cleaning
Titanic_clean <- Titanic %>% 
  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>% 
    na.omit()
head(Titanic_clean)

# Create k5 CV for the data
titanic_c_cv5 <- vfold_cv(Titanic_clean, v = 5)
```

For the classification part, we use the [Titanic dataset](https://www.kaggle.com/c/titanic). This data set is basic information about the passengers on the Titanic. In this dataset:

+ `Survived`: if the passenger is survival(0 = No, 1 = Yes)
+ `Pclass`: the ticket class(1 = 1st, 2 = 2nd, 3 = 3rd)
+ `Sex`: the gender of the passenger
+ `Age`: age in years
+ `SibSp`: the number of siblings / spouses aboard the Titanic
+ `Parch`: the number of parents / children aboard the Titanic
+ `Fare`: the passenger fare
+ `Embarked`: the port of Embarkation(C = Cherbourg, Q = Queenstown, S = Southampton)

The dataset is from Kaggle.

### 2 Research Questions

(From Kaggle:) On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.

While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.

In this challenge, we want to build a classification model that answers the question: “what sorts of people were more likely to survive?” using the dataset. The graph below is the distribution of `Survived`.

```{r}
# Visualize the outcome variable
Titanic_clean %>% 
  ggplot(aes(x = as.factor(Survived))) + 
  geom_bar() +
  xlab("Survived")
```

### 3 Methods

```{r}
# Process the data and make the recipe for classification
titanic_c <- Titanic_clean %>%
  mutate(Survived = as.factor(Survived))

rec_c <- recipe(Survived ~ ., data = titanic_c) %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(all_numeric_predictors())
```

```{r}
# Helper method to calculate Sensitivity, Specificity, and Accuracy
helper_confusion <- function(x) {
  return(list(
    "Sensitivty" = x$table[1] / (x$table[1] + x$table[2]),
    "Specificity" = x$table[4] / (x$table[3] + x$table[4]),
    "Accuracy" = (x$table[1] + x$table[4]) / (x$table[1] + x$table[2] + x$table[3] + x$table[4])
  ))
}
```

#### 3.1 Random Forest

```{r}
# model spec for RF
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>%
  set_args(
    mtry = NULL,
    trees = 100,
    min_n = 2,
    probability = FALSE,
    importance = 'impurity'
  ) %>%
  set_mode('classification')

# workflow for RF
rf_wf <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 2)) %>%
  add_recipe(rec_c)
```

```{r}
# Fit the model
rf_fit <- fit(rf_wf, data = titanic_c)
```

```{r}
# Get the OOB output
rf_OOB_output <- function(fit_model, truth) {
  tibble(
    .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'),
    #OOB predictions
    class = truth
  )
}

rf_output <- rf_OOB_output(rf_fit, titanic_c %>% pull(Survived))

rf_OOB <- rf_output %>%
  accuracy(truth = class, estimate = .pred_class)
```

```{r}
rf_output2 <- rf_wf %>%
  update_model(rf_spec %>% set_args(importance = "permutation")) %>%
  fit(data = titanic_c) %>%
  extract_fit_engine()

rf_output2 %>%
  vip(num_features = 30) + theme_classic()
```

```{r}
rf_output2 %>% vip::vi() %>% head()
rf_output2 %>% vip::vi() %>% tail()
```

```{r}
rf_confusion <- rf_OOB_output(rf_fit, titanic_c %>% pull(Survived)) %>%
  conf_mat(truth = class, estimate = .pred_class)
rf_confusion
```

```{r}
helper_confusion(rf_confusion)
```

The first classification model we use is random forest.

To evaluate the Random Forest model, we utilized OOB evaluation to avoid the computational intensity of cross-validation in this case. Based on the results of the OOB evaluation, we output a confusion matrix to get the values of metrics such as accuracy, sensitivity and specificity. The goals of using Random Forest were to observe the variable importance of the many factors that could have affected the likelihood of survival of a particular passenger. 

Conclusion for RF: The model generally shows an acceptable performance with more than 82% accuracy. The model does a good job at predicting the correct/real outcome. We do not that the model does do relatively poorly at predicting those who survived, as the false negatives in our result are rather high (at 86), for which reason the specificity is relatively lower. This means that our model performs better at predicting the outcome of the passenger that did not survive, in comparison to a passenger who did survive.

#### 3.2 KNN

```{r}
knn_spec <- 
  nearest_neighbor() %>%
  set_args(neighbors = tune()) %>% 
  set_engine(engine = 'kknn') %>%
  set_mode('classification') 

knn_wf <- workflow() %>%
  add_model(knn_spec) %>% 
  add_recipe(rec_c)
```

```{r}
penalty_grid <- grid_regular(neighbors(range = c(1, 100)),
                             levels = 20)

knn_cv <- tune_grid(knn_wf,
                    resamples = titanic_c_cv5,
                    grid = penalty_grid)

knn_cv %>%
  collect_metrics() %>%
  ggplot(aes(x = neighbors,
             y = mean,
             color = .metric)) +
  geom_line()
```

```{r}
k <- knn_cv %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  filter(mean == max(mean)) %>% 
  pull(neighbors)

knn_spec2 <- 
  nearest_neighbor() %>%
  set_args(neighbors = k) %>% 
  set_engine(engine = 'kknn') %>%
  set_mode('classification')

knn_wf2 <- workflow() %>%
  add_model(knn_spec2) %>% 
  add_recipe(rec_c)

knn_fit <- fit(knn_wf2, titanic_c)

knn_output <- knn_fit %>%
  predict(new_data = titanic_c) %>%
  bind_cols(titanic_c)

knn_confusion <- knn_output %>% 
  conf_mat(truth = Survived, estimate= .pred_class)

helper_confusion(knn_confusion)
```

The second classification model we use is KNN. 

To evaluate the KNN model, we utilized CV to select the best tuning parameters. Based on the results shown in the graph above, we select k = 21 for the number of neighbors. Then, we get the prediction based on KNN model for k = 21, and calculate the metrics. 

Conclusion for KNN: The model generally shows an acceptable performance with more than 87% accuracy. The model does a good job at predicting the correct/real outcome. We do not that the model does do relatively poorly at predicting those who survived, , for which reason the specificity is relatively lower. This means that, similar to RF, our model performs better at predicting the outcome of the passenger that did not survive, in comparison to a passenger who did survive.

### 4 Evaluation

```{r}
cbind(RF = helper_confusion(rf_confusion)) %>% 
  cbind(KNN = helper_confusion(knn_confusion))
```

Using accuracy (cross validated in case of KNN and OOB in case of RF) as our metric (since in this context, sensitivity and specificity are equally important), we see that KNN classification performs better than Random forest as it yields a larger accuracy than RF. However, it is important to note that KNN Classification is more prone to overfitting than RF, and should we fit both models to unseen data, we may observe superior performance from RF.(**Need More**)

### 5 Conclusion




## Part 3: Clustering

### Data Context

### Research Questions

```{r}

```










