---
title: "STAT-253_Final"
author: "Aalyan Mahmood, Eric Wang"
date: "4/26/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE, tidy = TRUE)
```

```{r}
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(vip)
library(ranger)
library(rpart.plot)
library(broom)
library(kknn)
library(glmnet)
library(ISLR)

tidymodels_prefer()
theme_set(theme_bw())

set.seed(123)
```

## Part 1: Regression

### 1 Data Context

```{r}
data("Credit")

# Data Cleaning
Credit <- Credit %>% 
  na.omit() %>% 
  select(-ID, -Rating)
head(Credit)

# Create k5 CV for the data
Credit_cv5 <- vfold_cv(Credit, v = 5)
```

For the regression part, we use a simulated data set containing information on 400 customers. In this dataset:

+ `Income`: the individual's income in $1,000's
+ `Limit`: the credit limit
+ `Cards`: the number of credit cards
+ `Age`: the age in years
+ `Education`: the number of years of eduction
+ `Gender`: a factor with levels Male and Female
+ `Student`: a factor with levels No and Yes indicating whether the individual was a student
+ `Married`: a factor with levels No and Yes indicating whether the individual was married
+ `Ethnicity`: a factor with levels African American, Asian, and Caucasian indicating the individual's ethnicity
+ `Balance`: the average credit card balance in dollar
  
The dataset is simulated data, with thanks to Albert Kim for pointing out that this was omitted, and supplying the data and man documentation page on Oct 19, 2017.

### 2 Research Questions

We fit regression models to predict individual's credit limit in dollar. The graph below is the distribution of `Limit`.

```{r}
# Visualize the outcome variable
Credit %>% 
  ggplot(aes(x = Limit)) + 
  geom_histogram()
```

### 3 Methods

```{r}
# Process the data and make the recipe for regression
rec_r <- recipe(Limit ~ ., data = Credit) %>%
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_corr(all_numeric_predictors())
```

#### 3.1 OLS

```{r}
# model spec for OLS
lm_spec <-
  linear_reg() %>%
  set_engine(engine = "lm") %>%
  set_mode("regression")

# workflow for OLS
lm_wf <- workflow() %>%
  add_recipe(rec_r) %>%
  add_model(lm_spec) 
```

```{r}
# Fit the OLS model
lm_fit <- fit(lm_wf, Credit)
```

```{r}
# Present the OLS model result and its metrics
tidy(lm_fit)
glance(lm_fit)
```

```{r}
#  Calculate and collect CV metrics for OLS
lm_output <- fit_resamples(lm_wf,
                           resamples = Credit_cv5,
                           metrics = metric_set(rmse, rsq, mae))

collect_metrics(lm_output)
```

```{r}
# Visualize the residual plot for OLS
lm_output2 <- lm_fit %>%
  predict(new_data = Credit) %>%
  bind_cols(Credit) %>%
  mutate(resid = Limit - .pred)

ggplot(lm_output2, aes(x = .pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()
```

The first model we use is ordinary least square (linear regression). 

Based on 5-folds cross-validation, we get MAE(272.707), RMSE(365.188), $R^2$(0.975). Overall, as the range of the outcome value `Limit` is quite large (over 10000), MAE and RMSE for this model are not very large. Also, the $R^2$ indicates that the model has a good performance in predicting the outcome. 

We also use a residual plot to visualize the prediction. However, we can see that points are not randomly positioned on the plot, and there is still a significant difference between the red line and blue line. The residual plot shows that there the model is not good enough in general.

#### 3.2 LASSO

```{r}
# model spec for LASSO
lasso_spec <-
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>%
  set_engine(engine = 'glmnet') %>%
  set_mode('regression') 

# workflows for LASSO
lasso_wf <- workflow() %>%
  add_recipe(rec_r) %>%
  add_model(lasso_spec)
```

```{r}
# Fit the LASSO model
lasso_fit <- lasso_wf %>%
  fit(data = Credit)
```

```{r}
# Tune the LASSO model
glmnet_output <-
  lasso_fit %>% extract_fit_parsnip() %>% pluck('fit')

lambdas <- glmnet_output$lambda

coefs_lambdas <-
  coefficients(glmnet_output, s = lambdas)  %>%
  as.matrix() %>%
  t() %>%
  as.data.frame() %>%
  mutate(lambda = lambdas) %>%
  select(lambda, everything(),-`(Intercept)`) %>%
  pivot_longer(cols = -lambda,
               names_to = "term",
               values_to = "coef") %>%
  mutate(var = map_chr(stringr::str_split(term, "_"),  ~ .[1]))

coefs_lambdas %>%
  ggplot(aes(
    x = lambda,
    y = coef,
    group = term,
    color = var
  )) +
  geom_line() +
  theme_classic() +
  theme(legend.position = "bottom", legend.text = element_text(size = 8))
```

```{r}
# Visualize the best penalty term
penalty_grid <- grid_regular(penalty(range = c(-3, 3)),
                             levels = 20)

lasso_cv <- tune_grid(
  lasso_wf,
  resamples = Credit_cv5,
  metrics = metric_set(rmse, mae, rsq),
  grid = penalty_grid
)

autoplot(lasso_cv) + theme_classic()
```

```{r}
# Present the LASSO model result
best_penalty <- select_best(lasso_cv, metric = 'rmse')
lasso_wf2 <- finalize_workflow(lasso_wf, best_penalty)
lasso_fit <- fit(lasso_wf2, data = Credit)
tidy(lasso_fit)
```

```{r}
#  Calculate and collect CV metrics for LASSO
lasso_output <- lasso_cv %>% collect_metrics(summarize = TRUE) %>%
  na.omit() %>% 
  group_by(.metric) %>% 
  summarize(mean = mean(mean), std_err = mean(std_err))
lasso_output
```

```{r}
# Visualize the residual plot for LASSO
lasso_output2 <- lasso_fit %>%
  predict(new_data = Credit) %>%
  bind_cols(Credit) %>%
  mutate(resid = Limit - .pred)

ggplot(lasso_output2, aes(x = .pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()
```

The next method we use is LASSO. 

We plot a path graph for all the parameters. The longer it takes for the variable to become zero, the more important it is in the model. Thus, the graph shows that the most important variable is `Balance`, and the least important variables are `Gender` and `Ethnicity`. 

Next, we plot three graphs with the amount of regularization on the x-axis and statistics metrics on the y-axis. We then select the penalty number based on the best RMSE and fit the final model. We evaluate the model with MAE(337.641), RMSE(446.247), and $R^2$(0.970). In general, MAE and RMSE are not large, and $R^2$ is quite high.

We also make a residual plot to evaluate the model. Same to OLS, points are not randomly scattered around, and there is still a large difference between the blue and red lines.

#### 3.3 GAMs

```{r}
# GAM model using mgcv
gam_spec <-
  gen_additive_mod() %>%
  set_engine(engine = 'mgcv') %>%
  set_mode('regression')

gam_mod <- fit(gam_spec,
               Limit ~ s(Income) + Cards + Age + Education + Gender + Student + Married + Ethnicity + s(Balance),
               data = Credit)
```

```{r}
# Present the GAM model result
gam_mod %>% pluck('fit') %>% summary() 
```

```{r}
# Compare the result with the linear GAMs model
gam_mod_temp <- fit(gam_spec,
               Limit ~ Income + Cards + Age + Education + Gender + Student + Married + Ethnicity + Balance,
               data = Credit)
gam_mod_temp %>% pluck('fit') %>% summary() 
```

```{r}
# Visualize the evaluation plots for GAMs
par(mfrow = c(2, 3))
gam_mod %>% pluck('fit') %>% mgcv::gam.check()
gam_mod %>% pluck('fit') %>% plot()
```

```{r}
# Visualize the residual plot for GAMs
gam_output <- Credit %>%
  bind_cols(predict(gam_mod, new_data = Credit)) %>%
  mutate(resid = Limit - .pred)

ggplot(gam_output, aes(x = Income, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()

ggplot(gam_output, aes(x = Education, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()

ggplot(gam_output, aes(x = Balance, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()

ggplot(gam_output, aes(x = .pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  theme_classic()
```

```{r}
#  Calculate and collect CV metrics for GAMs
gam_spec2 <-
  linear_reg() %>%
  set_engine(engine = 'lm') %>%
  set_mode('regression')

gam_rec <- rec_r %>%
  step_naomit(all_numeric_predictors(), skip = FALSE) %>%
  step_ns(Income, deg_free = 1.481) %>% 
  step_ns(Balance, deg_free = 8.426)

gam_wf <- workflow() %>%
  add_model(gam_spec2) %>%
  add_recipe(gam_rec)

gam_output <- fit_resamples(gam_wf,
                            resamples = Credit_cv5,
                            metrics = metric_set(mae, rmse, rsq))

gam_output %>% collect_metrics()
```

The third model we use is GAMs. 

There are five numerical variables in the model: `Cards`, `Age`, `Education`, `Income`, and `Balance`, but the first three variables are essentially categorical variables, which are only expressed in numerical form. Therefore, we add splines to `Income`, `Education`, and `Balance`. Compared to the linear GAMs model, we can see the $R^2$ and deviance explained both increase, which means the added splines are effective.

We use "mgcv" engine to find the best edf and plot several graphs to see the results. We can see from those plots that GAMs work quite well. Especially for the last one, the residual plot, the blue and red lines basically overlap. It indicates that the model fits well. Based on the variable importance shown by the GAMs model output, `Cards` and `Student` are the most important variables, while `Married`, `Gender`, and `Ethnicity` are the least important variables.

From the perspective of metrics, MAE(149.967), RMSE(249.692), and $R^2$(0.988) show that the model has a good performance and compared to the previous two methods, it has a quite large improvement. 

### 4 Evaluations

```{r}
regreDF <- lm_output %>%
  collect_metrics() %>% 
  mutate(OLS = mean,
         OLS_std = std_err) %>% 
  select(.metric, OLS, OLS_std) %>% 
  left_join(lasso_output %>% 
              mutate(LASSO = mean,
                     LASSO_std = std_err) %>% 
              select(.metric, LASSO, LASSO_std), 
            by = ".metric") %>% 
  left_join(gam_output %>% 
              collect_metrics() %>% 
              mutate(GAMs = mean,
                     GAMs_std = std_err) %>% 
              select(.metric, GAMs, GAMs_std), 
            by = ".metric")

regreDF
```

For the evaluation metrics based on 5-folds cross-validation, LASSO has the highest MAE and RMSE and the lowest $R^2$, and GAMs achieve the lowest MAE and RMSE and the highest $R^2$. The MAEs and RMSEs in this situation are quite large, as the size of the outcome variable `Limit`, and these large metrics indicate that our outcomes have a large uncertainty and could lead to unfair credit card limits.

The residual plots for OLS and LASSO are quite similar, while that for GAMs shows a better performance, as the blue line and red line overlap more in the residual plot of the GAMs model. 

From the perspective of variable importance, the importance sequence (from high to low) for LASSO is `Balance`, `Income`, `Student`, and `Cards`, while `s(Balance)`, `s(Income)`, and `StudentYes` have the same variable importance in the GAMs model. Both the two sequences are reasonable intuitively. In reality, balance and income are two important indicators for credit card limits. Also, if a person is a student, that person is less able to repay the money, which means he or she is more likely to get a lower credit card limit.

Looking at the residual plots of all the models, there does seem to be some systematic bias because most of the residuals are focused in the the region below 6000 in the residual plots for all of our models. Given that the majority of the cases in our dataset are not students, this indicates a rather low income standing in the majority of the cases. 

Thus, the final model we choose is the GAMs model.

### 5 Conclusions

```{r}
gam_mod %>% pluck('fit') %>% summary() 
```

From the outcome above, we can see that our final model have five important indicators. `Intercept`, `Income`, and `Balance` show positive relationships with the final outcome `Limit`, which means a person with more income and balance would have a higher credit card limit. However, `StudentYes` and `Cards` show negative relationships with the outcome, meaning that when a person with more cards and being a student would have lower credit card limit. Mentioned above, the model has quite large MAE and RMSE, but $R^2$ is high, indicating that our model has a high probability to get a correct outcome. As the data is simulated, the model would not have a high sensibility to the real cases.

## Part 2: Classification

### 1 Data Context

```{r}
Titanic <- read_csv("Data/train.csv")

# Data Cleaning
Titanic_clean <- Titanic %>% 
  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>% 
    na.omit()
head(Titanic_clean)

set.seed(123)
```

For the classification part, we use the [Titanic dataset](https://www.kaggle.com/c/titanic). This data set is basic information about the passengers on the Titanic. In this dataset:

+ `Survived`: if the passenger is survival(0 = No, 1 = Yes)
+ `Pclass`: the ticket class(1 = 1st, 2 = 2nd, 3 = 3rd)
+ `Sex`: the gender of the passenger
+ `Age`: age in years
+ `SibSp`: the number of siblings / spouses aboard the Titanic
+ `Parch`: the number of parents / children aboard the Titanic
+ `Fare`: the passenger fare
+ `Embarked`: the port of Embarkation(C = Cherbourg, Q = Queenstown, S = Southampton)

The dataset is from Kaggle.

### 2 Research Questions

(From Kaggle:) On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.

While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.

In this challenge, we want to build a classification model that answers the question: “what sorts of people were more likely to survive?” using the dataset. The graph below is the distribution of `Survived`.

```{r}
# Visualize the outcome variable
Titanic_clean %>% 
  ggplot(aes(x = as.factor(Survived))) + 
  geom_bar() +
  xlab("Survived")
```

### 3 Methods

```{r}
# Process the data and make the recipe for classification
titanic_c <- Titanic_clean %>%
  mutate(Survived = as.factor(Survived))

rec_c <- recipe(Survived ~ ., data = titanic_c) %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(all_numeric_predictors())


# Create k5 CV for the data
titanic_c_cv5 <- vfold_cv(titanic_c, v = 5)
```

```{r}
# Helper method to calculate Sensitivity, Specificity, and Accuracy
helper_confusion <- function(x) {
  return(list(
    "Sensitivty" = x$table[1] / (x$table[1] + x$table[2]),
    "Specificity" = x$table[4] / (x$table[3] + x$table[4]),
    "Accuracy" = (x$table[1] + x$table[4]) / (x$table[1] + x$table[2] + x$table[3] + x$table[4])
  ))
}
```

#### 3.1 Random Forest

```{r}
# model spec for RF
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>%
  set_args(
    mtry = NULL,
    trees = 100,
    min_n = 2,
    probability = FALSE,
    importance = 'impurity'
  ) %>%
  set_mode('classification')

# workflow for RF
rf_wf <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 2)) %>%
  add_recipe(rec_c)
```

```{r}
# Fit the model
rf_fit <- fit(rf_wf, data = titanic_c)
```

```{r}
# Get the OOB output
rf_OOB_output <- function(fit_model, truth) {
  tibble(
    .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'),
    #OOB predictions
    class = truth
  )
}

rf_output <- rf_OOB_output(rf_fit, titanic_c %>% pull(Survived))

rf_OOB <- rf_output %>%
  accuracy(truth = class, estimate = .pred_class)
```

```{r}
rf_output2 <- rf_wf %>%
  update_model(rf_spec %>% set_args(importance = "permutation")) %>%
  fit(data = titanic_c) %>%
  extract_fit_engine()

rf_output2 %>%
  vip(num_features = 30) + theme_classic()
```

```{r}
rf_output2 %>% vip::vi() %>% head()
rf_output2 %>% vip::vi() %>% tail()
```

```{r}
rf_confusion <- rf_OOB_output(rf_fit, titanic_c %>% pull(Survived)) %>%
  conf_mat(truth = class, estimate = .pred_class)
rf_confusion
```

```{r}
helper_confusion(rf_confusion)
```

The first classification model we use is random forest.

To evaluate the Random Forest model, we utilized OOB evaluation to avoid the computational intensity of cross-validation in this case. Based on the results of the OOB evaluation, we output a confusion matrix to get the values of metrics such as accuracy, sensitivity and specificity. The goals of using Random Forest were to observe the variable importance of the many factors that could have affected the likelihood of survival of a particular passenger. 

Conclusion for RF: The model generally shows an acceptable performance with more than 80.76% accuracy. The model does a good job at predicting the correct/real outcome. We do note that the model does do relatively poorly at predicting those who survived, as the false negatives in our result are rather high (at 98), for which reason the specificity is relatively lower at 0.6597. This means that our model performs better at predicting the outcome of the passenger that did not survive, in comparison to a passenger who did survive.

#### 3.2 KNN

```{r}
knn_spec <- 
  nearest_neighbor() %>%
  set_args(neighbors = tune()) %>% 
  set_engine(engine = 'kknn') %>%
  set_mode('classification') 

knn_wf <- workflow() %>%
  add_model(knn_spec) %>% 
  add_recipe(rec_c)
```

```{r}
penalty_grid <- grid_regular(neighbors(range = c(1, 100)),
                             levels = 20)

knn_cv <- tune_grid(knn_wf,
                    resamples = titanic_c_cv5,
                    grid = penalty_grid)

knn_cv %>%
  collect_metrics() %>%
  ggplot(aes(x = neighbors,
             y = mean,
             color = .metric)) +
  geom_line()
```

```{r}
k <- knn_cv %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  filter(mean == max(mean)) %>% 
  pull(neighbors)

knn_spec2 <- 
  nearest_neighbor() %>%
  set_args(neighbors = k) %>% 
  set_engine(engine = 'kknn') %>%
  set_mode('classification')

knn_wf2 <- workflow() %>%
  add_model(knn_spec2) %>% 
  add_recipe(rec_c)

knn_fit <- fit(knn_wf2, titanic_c)

knn_output <- knn_fit %>%
  predict(new_data = titanic_c) %>%
  bind_cols(titanic_c)

knn_confusion <- knn_output %>% 
  conf_mat(truth = Survived, estimate= .pred_class)

helper_confusion(knn_confusion)
```

The second classification model we use is KNN. 

To evaluate the KNN model, we utilized CV to select the best tuning parameters. Based on the results shown in the graph above, we select $k = 21$ for the number of neighbors. Then, we get the prediction based on KNN model for $k = 21$, and calculate the metrics. 

Conclusion for KNN: The model generally shows an acceptable performance with more than 87% accuracy. The model does a good job at predicting the correct/real outcome. We do not that the model does do relatively poorly at predicting those who survived, , for which reason the specificity is relatively lower. This means that, similar to RF, our model performs better at predicting the outcome of the passenger that did not survive, in comparison to a passenger who did survive.

### 4 Evaluation

```{r}
cbind(RF = helper_confusion(rf_confusion)) %>% 
  cbind(KNN = helper_confusion(knn_confusion))
```

Using accuracy (cross validated in case of KNN and OOB in case of RF) as our metric (since in this context, sensitivity and specificity are equally important), we see that KNN classification performs better than Random forest as it yields a larger accuracy than RF. However, it is important to note that KNN Classification is more prone to overfitting than RF, and should we fit both models to unseen data, we may observe superior performance from RF.

### 5 Conclusion

We are persuaded to select KNN classification as our final model but given the above reasoning, vhoosing RF may be a better option in terms of avoiding overfitting. Considering the resulting confusion matrix from RF, we calculate the sensitivity to be 0.908, the specificity to be 0.6597 and the accuracy to be 0.8076. These metrics indicate to us a strong performance from our model, as stated before in the RF section, but we also acknowledge the weakness of the model in terms of predicting the positive (Survived) outcome. 
The data used is based on real cases from the titanic incident, for which reason the results are sensible, meaning that they do indicate patterns in survival of the real passengers aboard the Titanic, and give us a sense of what feature of a passenger (their class and gender most inportantly, given the variable importance plot) enhanced their chances of survival (a female passenger in first class for example would have higher odds of survival, compared to a male passenger in first class, and a first class passenger would have higher odds of survival than a second or third class passenger).

## Part 3: Clustering

### 1 Data Context

```{r}
# Data Cleaning
Pokemon_890 <- read_csv("Data/pokedex_(Update_05.20).csv") %>% 
  mutate(Name = name) %>% 
  select(-name)
Pokemon_721 <- read_csv("Data/Pokemon.csv")

Pokemon_final <- Pokemon_721 %>% 
  left_join(Pokemon_890,
            by = "Name") %>% 
  select(c(`Type 1`, HP, Attack, Defense, `Sp. Atk`, `Sp. Def`, Speed)) %>% 
  distinct()
head(Pokemon_final)
```


For the clustering part, the datasets we use are about Pokémon, a famous video game presented by Game Freak. The first dataset is from Kaggle([link](https://www.kaggle.com/abcsds/pokemon)) and includes 721 Pokémon, their number, name first and the second type, and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed. The second dataset is also from Kaggle ([link](https://www.kaggle.com/abcsds/pokemon))contains 890 Pokémon, with their English name, Japanese name, German name, the numbered generation in which the Pokémon was first introduced, the status or quality of the Pokémon, species, and their number of types. Two datasets are originally from the Pokémon official website. We combine two datasets based on their shared Pokémon, which means the number of Pokémon for the final dataset would be 721, with all the variables in two original datasets.

+ `Type 1`: the type of the Pokémon
+ `HP`: the HP of the Pokémon
+ `Attack`: the attack of the Pokémon
+ `Defense`: the defense of the Pokémon
+ `Sp. Atk`: the special attack of the Pokémon
+ `Sp. Def`: the special defense of the Pokémon
+ `Speed`: the speed of the Pokémon

### 2 Research Questions

With K-Means Clustering, the topic we wanted to address was in regards to team selection. Selecting a diverse and all rounded team is the goal of any player, and so constructing a team by utilizing k-Means Clustering is our aim. Therefore, our research question is: "What Pokemon types perform the best in different aspects of battle, i.e in terms of defense, attack, bulk and speed?" To answer this question, we aim to create different clusters with different attributes (like attack and special attack, etc.) to output the type that displays the highest base statistic points in the field. 

### 3 Clustering

```{r}
Pokemon_1 <- Pokemon_final %>% 
  select(Attack, `Sp. Atk`)
```

```{r}
# Data-specific function to cluster and calculate total within-cluster SS
pokemon_cluster_ss <- function(k){
    # Perform clustering
    kclust <- kmeans(scale(Pokemon_1), centers = k)

    # Return the total within-cluster sum of squares
    return(kclust$tot.withinss)
}

tibble(
    k = 1:15,
    tot_wc_ss = purrr::map_dbl(1:15, pokemon_cluster_ss)
) %>% 
    ggplot(aes(x = k, y = tot_wc_ss)) +
    geom_point() + 
    labs(x = "Number of clusters",y = 'Total within-cluster sum of squares') + 
    theme_classic()
```

```{r}
kclust_1_k4 <- kmeans(Pokemon_1, centers = 4)
Pokemon_1 <- Pokemon_1 %>%
    mutate(kclust_4 = factor(kclust_1_k4$cluster))

Pokemon_1 %>% 
  ggplot(aes(x = Attack,
             y = `Sp. Atk`,
             color = kclust_4)) +
  geom_point()
```

```{r}
Pokemon_final %>% 
  mutate(kclust_4 = factor(kclust_1_k4$cluster)) %>% 
  group_by(kclust_4, `Type 1`) %>% 
  summarize(sum = n())
```

```{r}
Pokemon_2 <- Pokemon_final %>% 
  select(Defense, `Sp. Def`)

# Data-specific function to cluster and calculate total within-cluster SS
pokemon_cluster_ss <- function(k){
    # Perform clustering
    kclust <- kmeans(scale(Pokemon_2), centers = k)

    # Return the total within-cluster sum of squares
    return(kclust$tot.withinss)
}

tibble(
    k = 1:15,
    tot_wc_ss = purrr::map_dbl(1:15, pokemon_cluster_ss)
) %>% 
    ggplot(aes(x = k, y = tot_wc_ss)) +
    geom_point() + 
    labs(x = "Number of clusters",y = 'Total within-cluster sum of squares') + 
    theme_classic()

kclust_2_k4 <- kmeans(Pokemon_2, centers = 4)
Pokemon_2 <- Pokemon_2 %>%
    mutate(kclust_4 = factor(kclust_2_k4$cluster))

Pokemon_2 %>% 
  ggplot(aes(x = Defense,
             y = `Sp. Def`,
             color = kclust_4)) +
  geom_point()
```

```{r}
Pokemon_final %>% 
  mutate(kclust_4 = factor(kclust_2_k4$cluster)) %>% 
  group_by(kclust_4, `Type 1`) %>% 
  summarize(sum = n())
```

```{r}
Pokemon_3 <- Pokemon_final %>% 
  select(`Sp. Atk`, Speed)

# Data-specific function to cluster and calculate total within-cluster SS
pokemon_cluster_ss <- function(k){
    # Perform clustering
    kclust <- kmeans(scale(Pokemon_3), centers = k)

    # Return the total within-cluster sum of squares
    return(kclust$tot.withinss)
}

tibble(
    k = 1:15,
    tot_wc_ss = purrr::map_dbl(1:15, pokemon_cluster_ss)
) %>% 
    ggplot(aes(x = k, y = tot_wc_ss)) +
    geom_point() + 
    labs(x = "Number of clusters",y = 'Total within-cluster sum of squares') + 
    theme_classic()

kclust_3_k4 <- kmeans(Pokemon_3, centers = 4)
Pokemon_3 <- Pokemon_3 %>%
    mutate(kclust_4 = factor(kclust_3_k4$cluster))

Pokemon_3 %>% 
  ggplot(aes(x = `Sp. Atk`,
             y = Speed,
             color = kclust_4)) +
  geom_point()
```

```{r}
Pokemon_final %>% 
  mutate(kclust_4 = factor(kclust_3_k4$cluster)) %>% 
  group_by(kclust_4, `Type 1`) %>% 
  summarize(sum = n())
```

```{r}
Pokemon_4 <- Pokemon_final %>% 
  select(Defense, HP)

# Data-specific function to cluster and calculate total within-cluster SS
pokemon_cluster_ss <- function(k){
    # Perform clustering
    kclust <- kmeans(scale(Pokemon_4), centers = k)

    # Return the total within-cluster sum of squares
    return(kclust$tot.withinss)
}

tibble(
    k = 1:15,
    tot_wc_ss = purrr::map_dbl(1:15, pokemon_cluster_ss)
) %>% 
    ggplot(aes(x = k, y = tot_wc_ss)) +
    geom_point() + 
    labs(x = "Number of clusters",y = 'Total within-cluster sum of squares') + 
    theme_classic()

kclust_4_k4 <- kmeans(Pokemon_4, centers = 4)
Pokemon_4 <- Pokemon_4 %>%
    mutate(kclust_4 = factor(kclust_4_k4$cluster))

Pokemon_4 %>% 
  ggplot(aes(x = Defense,
             y = HP,
             color = kclust_4)) +
  geom_point()
```

```{r}
Pokemon_final %>% 
  mutate(kclust_4 = factor(kclust_4_k4$cluster)) %>% 
  group_by(kclust_4, `Type 1`) %>% 
  summarize(sum = n())
```

### 4 Evaluation

To choose the most appropriate value of k, we created a plot of number of clusters against Total within cluster sum of squares, and chose the value at the elbow of the plot for k, which turned out to be $k=4$.

After choosing the most appropriate value for k (the value at the elbow of the number of clusters plot), we then plotted different variables against each other and formed clusters. Next, we output the number Pokemon of each type in each cluster to observe which type was dominant in a particular field (e.g best attacker would be one with the highest attack and special attack). This means that we selected a different set of 2 variables to explore different characteristics (different set of variables for the distance measure). 


With all this set up, we observed that the strongest attacker type was, unsurprisingly, Dragon as dragon types were in the highest quantity in the cluster 1 of Attack vs Sp. Attack. The weakest type here was bug.

Next, when looking at the best defensive ability, we observed that the best type for defense (i.e in most quantity in cluster 2 for Defense vs Sp. defense) was rock or steel, and this is rather consistent with previous assumptions as Pokemon such as Aggron (A rock and steel type) boast a really high defense base stat. The weakest again here was bug type.

Lastly, we wanted to see which type would be the most tanky, in the sense that it can sustain an attack (high defense) and can take a lot of hits (high hp stat), and the cluster and visualization for this suggested that Normal types had a good balance of both these statistics (observed from cluster 3), and we can think of Pokemon which back this result up (such as Snorlax). 

Therefore as a final insight, we were able to use K-Means clustering to form the perfect Pokemon team, which would consist of a Dragon type (such as Salamence), a tanky normal type such as Snorlax to hold off opponents which we cannot combat with super effective moves, a rock type (and preferably steel dual type) to sustain hits, and then a mix of other types such as psychic which also boasts high attack. However, our strategy shows us very clearly that bug types ar to be avoided, and that is also the general consensus in the Pokemon community.







